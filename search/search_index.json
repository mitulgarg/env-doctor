{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Env-Doctor","text":"<p>Env-Doctor is a GPU environment diagnostic tool for Python AI/ML developers.</p> <p>It detects and fixes the most common source of broken GPU setups: version mismatches between your NVIDIA driver, CUDA toolkit, cuDNN, and Python libraries like PyTorch, TensorFlow, and JAX.</p> <p>Run one command. Get a full diagnosis. Get the exact fix.</p> <pre><code>pip install env-doctor\nenv-doctor check\n</code></pre> <p>Common symptom: <code>torch.cuda.is_available()</code> returns <code>False</code> right after installing PyTorch \u2014 because your driver supports CUDA 11.8, but <code>pip install torch</code> silently pulled CUDA 12.4 wheels.</p> <p>Env-Doctor also checks GPU architecture compatibility, Python version conflicts, Docker GPU configs, AI model VRAM requirements, and exposes all diagnostics to AI assistants via a built-in MCP server.</p>"},{"location":"#features","title":"Features","text":"Feature What It Does One-Command Diagnosis Instantly check compatibility between GPU Driver \u2192 CUDA Toolkit \u2192 cuDNN \u2192 PyTorch/TensorFlow/JAX Compute Capability Check Detect GPU architecture mismatches \u2014 catches why <code>torch.cuda.is_available()</code> returns <code>False</code> on new GPUs (e.g. Blackwell RTX 5000) even when driver and CUDA are healthy Python Version Compatibility Detect Python version conflicts with AI libraries and dependency cascade impacts CUDA Installation Guide Get platform-specific, copy-paste CUDA installation commands for Ubuntu, Debian, RHEL, Fedora, WSL2, Windows, and Conda Deep CUDA Analysis Reveals multiple installations, PATH issues, environment misconfigurations Compilation Guard Warns if system <code>nvcc</code> doesn't match PyTorch's CUDA \u2014 preventing flash-attention build failures WSL2 GPU Support Detects WSL1/WSL2 environments, validates GPU forwarding, catches common driver conflicts for WSL2 Safe Install Commands Prescribes the exact <code>pip install</code> command that works with YOUR driver Container Validation Catches GPU config errors in Dockerfiles and docker-compose with DB-driven recommendations AI Model Compatibility Check if your GPU can run any model (LLMs, Diffusion, Audio) before downloading cuDNN Detection Finds cuDNN libraries, validates symlinks, checks version compatibility MCP Server Expose all diagnostics to AI assistants (Claude Code, Claude Desktop, Zed) via Model Context Protocol stdio \u2014 no browser needed Migration Helper Scans code for deprecated imports (LangChain, Pydantic) and suggests fixes"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install env-doctor\n</code></pre>"},{"location":"#quick-start-other-commands-explained-extensively-seperately","title":"Quick start (Other commands explained extensively seperately)","text":"<pre><code># Diagnose your environment\nenv-doctor check\n\n# Check Python version compatibility\nenv-doctor python-compat\n\n# Get CUDA installation instructions\nenv-doctor cuda-install\n\n# Get safe install command for PyTorch\nenv-doctor install torch\n\n# Check if a model fits on your GPU\nenv-doctor model llama-3-8b\n</code></pre>"},{"location":"#cli-demo-environment-check","title":"CLI Demo \u2014 Environment Check","text":""},{"location":"#cli-demo-model-checker","title":"CLI Demo \u2014 Model Checker","text":""},{"location":"#mcp-server-demo-claude-code-in-action","title":"MCP Server Demo \u2014 Claude Code in Action","text":"<p>Use env-doctor as an MCP server and let your AI assistant diagnose GPU environments, fetch safe install commands, and validate Dockerfiles \u2014 all without leaving the chat.</p> <p> MCP Integration Guide</p>"},{"location":"#star-history","title":"Star History","text":""},{"location":"#whats-next","title":"What's Next?","text":"<ul> <li> <p> Getting Started</p> <p>Complete installation and first steps guide</p> <p> Getting Started</p> </li> <li> <p> Commands</p> <p>Full reference for all CLI commands</p> <p> Commands</p> </li> <li> <p> Container Validation</p> <p>Validate Dockerfiles and docker-compose for GPU issues</p> <p> Dockerfile Validation</p> </li> <li> <p> Model Compatibility</p> <p>Check if AI models fit on your GPU before downloading</p> <p> Model Checker</p> </li> </ul>"},{"location":"#frequently-asked-questions","title":"Frequently Asked Questions","text":"Why does <code>torch.cuda.is_available()</code> return <code>False</code>? <p>The most common causes are:</p> <ol> <li>Your GPU's SM architecture isn't in your PyTorch wheel (common on new GPUs like Blackwell RTX 5000)</li> <li>Your NVIDIA driver is too old for your CUDA toolkit</li> <li>CUDA version mismatch between driver, toolkit, and PyTorch</li> </ol> <p>Run <code>env-doctor check</code> to get the exact cause and fix.</p> How do I fix a CUDA version mismatch with PyTorch? <p>Run <code>env-doctor check</code> to identify what's mismatched, then <code>env-doctor install torch</code> to get the exact <code>pip install</code> command with the correct <code>--index-url</code> for your driver.</p> Why does flash-attention fail to build? <p>flash-attn requires an exact match between your system <code>nvcc</code> version and PyTorch's CUDA build. Run <code>env-doctor install flash-attn</code> \u2014 it detects the mismatch and gives you two fix paths.</p> How do I use env-doctor with Claude or other AI assistants? <p>Env-doctor ships a built-in MCP server (<code>env-doctor-mcp</code>). Add it to your Claude Desktop or Claude Code config and your AI assistant can call all diagnostic tools directly from the chat. See the MCP Integration Guide.</p> My new RTX 5000 / Blackwell GPU isn't working with PyTorch. What do I do? <p>Stable PyTorch wheels don't yet include SM 120 (Blackwell) support. Run <code>env-doctor check</code> \u2014 it detects whether you have a hard or soft architecture mismatch and provides the exact nightly PyTorch install command with <code>sm_120</code> support.</p>"},{"location":"ADDING_MODELS/","title":"Adding Models to env-doctor","text":"<p>This guide explains how to add new AI models to env-doctor's model compatibility database.</p>"},{"location":"ADDING_MODELS/#quick-start","title":"Quick Start","text":"<p>Models are stored in <code>src/env_doctor/data/model_requirements.json</code>. To add a model, you only need its parameter count \u2014 everything else is optional!</p>"},{"location":"ADDING_MODELS/#minimum-required-data","title":"Minimum Required Data","text":"<pre><code>{\n  \"your-model-name\": {\n    \"params_b\": 7.0,\n    \"category\": \"llm\",\n    \"family\": \"your-model-family\"\n  }\n}\n</code></pre> <ul> <li>model-name (key): Use lowercase with hyphens (e.g., <code>llama-3-8b</code>)</li> <li>params_b: Model size in billions of parameters (e.g., 7.0 for 7 billion)</li> <li>category: One of: <code>llm</code>, <code>diffusion</code>, <code>audio</code>, <code>language</code></li> <li>family: Model family (e.g., <code>llama-3</code>, <code>mistral</code>, <code>stable-diffusion</code>)</li> </ul>"},{"location":"ADDING_MODELS/#full-entry-example","title":"Full Entry Example","text":"<p>For maximum accuracy, include measured VRAM values:</p> <pre><code>{\n  \"llama-3-8b\": {\n    \"params_b\": 8.0,\n    \"category\": \"llm\",\n    \"family\": \"llama-3\",\n    \"hf_id\": \"meta-llama/Meta-Llama-3-8B\",\n    \"vram\": {\n      \"fp16\": 19200,\n      \"int4\": 4800\n    },\n    \"notes\": \"Instruction-tuned variant, best for instruction following\"\n  }\n}\n</code></pre> <p>Optional fields: - hf_id: HuggingFace model ID (for reference links) - vram: Measured VRAM in MB for specific precisions (see below) - notes: Implementation details or usage notes</p>"},{"location":"ADDING_MODELS/#finding-parameter-counts","title":"Finding Parameter Counts","text":""},{"location":"ADDING_MODELS/#option-1-huggingface-model-card-recommended","title":"Option 1: HuggingFace Model Card (Recommended)","text":"<ol> <li>Visit the model on HuggingFace: <code>https://huggingface.co/[author]/[model-name]</code></li> <li>Look for the parameter count in:</li> <li>Model description/tags (often shows \"70B\" or \"Parameters: 70 billion\")</li> <li>Model card text</li> <li>Technical specs section</li> </ol> <p>Example search: <pre><code>https://huggingface.co/meta-llama/Meta-Llama-3-70B\n</code></pre></p>"},{"location":"ADDING_MODELS/#option-2-google-search","title":"Option 2: Google Search","text":"<pre><code>\"[model name] parameters billion\" OR \"[model name] size\"\n</code></pre> <p>Example: <code>\"Llama 3 70B parameters\"</code> \u2192 Result: \"70 billion\"</p>"},{"location":"ADDING_MODELS/#option-3-github-repository","title":"Option 3: GitHub Repository","text":"<p>Check the model's official GitHub repository: - Look in README.md - Check model_config.json files - Search for \"params\" or \"parameters\"</p>"},{"location":"ADDING_MODELS/#option-4-official-documentation","title":"Option 4: Official Documentation","text":"<p>Most model providers publish parameter counts in: - Official model cards - Research papers (usually in abstract or table) - Blog posts announcing the model</p>"},{"location":"ADDING_MODELS/#adding-measured-vram-values-optional-advanced","title":"Adding Measured VRAM Values (Optional, Advanced)","text":"<p>If you have access to the model and a GPU, you can measure actual VRAM usage for more accuracy.</p>"},{"location":"ADDING_MODELS/#how-to-measure-vram","title":"How to Measure VRAM","text":"<ol> <li>Load model in specific precision:</li> </ol> <pre><code>import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"meta-llama/Meta-Llama-3-8B\"\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=torch.float16,  # or torch.bfloat16, torch.float32\n    device_map=\"auto\"\n)\n</code></pre> <ol> <li>Check VRAM usage:</li> </ol> <pre><code>nvidia-smi\n</code></pre> <p>Look for the GPU memory usage of your process.</p> <ol> <li>Record the value:</li> </ol> <pre><code>{\n  \"vram\": {\n    \"fp16\": 19200,  # in MB\n    \"int4\": 4800\n  }\n}\n</code></pre>"},{"location":"ADDING_MODELS/#vram-values-format","title":"VRAM Values Format","text":"<ul> <li>Key: Precision (fp32, fp16, bf16, int8, int4, fp8)</li> <li>Value: VRAM in MB (megabytes), not GB</li> </ul> <p>Example conversions: - 19.2 GB = 19,200 MB - 4.8 GB = 4,800 MB - 140 GB = 140,000 MB</p>"},{"location":"ADDING_MODELS/#which-precisions-to-measure","title":"Which Precisions to Measure?","text":"<p>Start with the most common: 1. fp16 - Standard inference precision 2. int4 - Quantized, most popular for memory efficiency 3. int8 - 8-bit quantized 4. bf16 - Brain float (if model supports it)</p>"},{"location":"ADDING_MODELS/#model-categories","title":"Model Categories","text":"<p>Choose the most appropriate category:</p> Category Examples Use When <code>llm</code> Llama, Mistral, Qwen, Mixtral Language model for text generation <code>diffusion</code> Stable Diffusion, FLUX, Kandinsky Image generation or image manipulation <code>audio</code> Whisper, Musicgen Speech recognition, generation, or processing <code>language</code> BERT, T5, RoBERTa Text encoding, classification, or small language tasks"},{"location":"ADDING_MODELS/#model-naming-conventions","title":"Model Naming Conventions","text":"<p>Do: - Use lowercase: <code>llama-3-8b</code> \u2705 - Use hyphens: <code>stable-diffusion-xl</code> \u2705 - Include size: <code>mixtral-8x7b</code> \u2705 - Be descriptive: <code>bert-base-uncased</code> \u2705</p> <p>Don't: - Use spaces: <code>llama 3 8b</code> \u274c - Use underscores for separation: <code>llama_3_8b</code> \u274c - Abbreviate: <code>sd-xl</code> instead of <code>stable-diffusion-xl</code> \u274c - Include version number: <code>v1</code>, <code>v2.0</code> \u274c (unless it's part of official name)</p>"},{"location":"ADDING_MODELS/#adding-aliases","title":"Adding Aliases","text":"<p>Help users find models with alternative names:</p> <pre><code>{\n  \"aliases\": {\n    \"llama3-8b\": \"llama-3-8b\",\n    \"sdxl\": \"stable-diffusion-xl\",\n    \"mistral-7b-v01\": \"mistral-7b\"\n  }\n}\n</code></pre> <p>Aliases are case-insensitive and automatically resolved.</p>"},{"location":"ADDING_MODELS/#database-schema-validation","title":"Database Schema Validation","text":"<p>All submitted models must pass validation:</p> <pre><code># Run validation tests\npytest tests/unit/test_vram_calculator.py::TestVRAMCalculatorDatabaseIntegrity -v\n</code></pre> <p>Automatic checks: - \u2705 All models have <code>params_b</code> &gt; 0 - \u2705 All <code>category</code> values are valid - \u2705 All aliases point to existing models - \u2705 VRAM values are reasonable (0 &lt; x &lt; 1,000,000 MB) - \u2705 Parameter counts are within realistic ranges</p>"},{"location":"ADDING_MODELS/#submitting-your-changes","title":"Submitting Your Changes","text":""},{"location":"ADDING_MODELS/#step-1-create-your-branch","title":"Step 1: Create Your Branch","text":"<pre><code>git checkout -b feature/add-models\n</code></pre>"},{"location":"ADDING_MODELS/#step-2-edit-the-database","title":"Step 2: Edit the Database","text":"<p>Add your models to <code>src/env_doctor/data/model_requirements.json</code>:</p> <pre><code>nano src/env_doctor/data/model_requirements.json\n# or use your preferred editor\n</code></pre>"},{"location":"ADDING_MODELS/#step-3-test-your-changes","title":"Step 3: Test Your Changes","text":"<pre><code># Run database validation tests\npytest tests/unit/test_vram_calculator.py::TestVRAMCalculatorDatabaseIntegrity -v\n\n# Test the model works via CLI\nenv-doctor model your-model-name\nenv-doctor model --list | grep your-model-name\n</code></pre>"},{"location":"ADDING_MODELS/#step-4-commit-and-push","title":"Step 4: Commit and Push","text":"<pre><code>git add src/env_doctor/data/model_requirements.json\ngit commit -m \"feat: add [model-name] and variants to model database\"\ngit push origin feature/add-models\n</code></pre>"},{"location":"ADDING_MODELS/#step-5-create-a-pull-request","title":"Step 5: Create a Pull Request","text":"<p>Include in your PR description: - Which models were added - Parameter counts and sources - Any measured VRAM values included - Models tested (if you have GPU access)</p> <p>Example PR title: <pre><code>feat: add Llama-3 variants (8B, 70B, 405B) to model database\n</code></pre></p>"},{"location":"ADDING_MODELS/#common-questions","title":"Common Questions","text":""},{"location":"ADDING_MODELS/#q-what-if-i-dont-know-exact-parameter-count","title":"Q: What if I don't know exact parameter count?","text":"<p>A: Use a reasonable estimate or measurement. For example: - Look at model size comparisons to known models - Check papers or documentation - Estimate from download size (rough: GB file size \u00d7 2 \u2248 parameters)</p>"},{"location":"ADDING_MODELS/#q-can-i-add-multiple-models-in-one-pr","title":"Q: Can I add multiple models in one PR?","text":"<p>A: Yes! Grouping related models (e.g., all Llama-3 variants) is encouraged.</p>"},{"location":"ADDING_MODELS/#q-how-do-i-find-vram-requirements-if-i-dont-have-a-gpu","title":"Q: How do I find VRAM requirements if I don't have a GPU?","text":"<p>A: The formula-based calculation is good enough for initial release. Many users measure and contribute VRAM values later.</p>"},{"location":"ADDING_MODELS/#q-what-if-a-model-has-multiple-variants-quantized-instruct-chat","title":"Q: What if a model has multiple variants (quantized, instruct, chat)?","text":"<p>A: Add the base model. If variants have significantly different parameter counts, add them separately:</p> <pre><code>{\n  \"llama-3-8b\": { \"params_b\": 8.0, ... },\n  \"llama-3-8b-instruct\": { \"params_b\": 8.0, ... }\n}\n</code></pre>"},{"location":"ADDING_MODELS/#q-how-do-i-find-the-huggingface-model-id","title":"Q: How do I find the HuggingFace model ID?","text":"<p>A: It's in the URL on HuggingFace:</p> <pre><code>https://huggingface.co/meta-llama/Meta-Llama-3-8B\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  &lt;- This is the hf_id\n</code></pre>"},{"location":"ADDING_MODELS/#q-whats-the-correct-value-for-family","title":"Q: What's the correct value for \"family\"?","text":"<p>A: Use the model series name, lowercase with hyphens: - <code>llama-3</code> (not <code>llama3</code> or <code>llama 3</code>) - <code>stable-diffusion</code> (not <code>sd</code> or <code>stable-diff</code>) - <code>mistral</code> (single models don't need numbers)</p>"},{"location":"ADDING_MODELS/#examples","title":"Examples","text":""},{"location":"ADDING_MODELS/#adding-a-single-model","title":"Adding a Single Model","text":"<pre><code>{\n  \"qwen-7b\": {\n    \"params_b\": 7.0,\n    \"category\": \"llm\",\n    \"family\": \"qwen\",\n    \"hf_id\": \"Qwen/Qwen-7B\"\n  }\n}\n</code></pre>"},{"location":"ADDING_MODELS/#adding-a-model-family-with-variants","title":"Adding a Model Family with Variants","text":"<pre><code>{\n  \"mixtral-8x7b\": {\n    \"params_b\": 46.7,\n    \"category\": \"llm\",\n    \"family\": \"mixtral\",\n    \"hf_id\": \"mistralai/Mixtral-8x7B-v0.1\",\n    \"notes\": \"Mixture of Experts: 46.7B total, 12.9B active\"\n  },\n  \"mixtral-8x22b\": {\n    \"params_b\": 176.0,\n    \"category\": \"llm\",\n    \"family\": \"mixtral\",\n    \"hf_id\": \"mistral-community/Mixtral-8x22B-v0.1\",\n    \"vram\": {\n      \"fp16\": 263000\n    }\n  }\n}\n</code></pre>"},{"location":"ADDING_MODELS/#adding-with-measured-vram","title":"Adding with Measured VRAM","text":"<pre><code>{\n  \"stable-diffusion-xl\": {\n    \"params_b\": 3.5,\n    \"category\": \"diffusion\",\n    \"family\": \"stable-diffusion\",\n    \"hf_id\": \"stabilityai/stable-diffusion-xl-base-1.0\",\n    \"vram\": {\n      \"fp16\": 8000\n    },\n    \"notes\": \"SDXL base model, improved quality over v1.5\"\n  }\n}\n</code></pre>"},{"location":"ADDING_MODELS/#need-help","title":"Need Help?","text":"<ul> <li>\ud83d\udcda Check existing models in <code>src/env_doctor/data/model_requirements.json</code></li> <li>\ud83d\udc1b Open an issue if you find errors</li> <li>\ud83d\udcac Discuss in GitHub discussions for questions</li> <li>\ud83d\udd17 Reference HuggingFace model cards for official specs</li> </ul>"},{"location":"ADDING_MODELS/#contribution-recognition","title":"Contribution Recognition","text":"<p>Thank you for contributing! Your additions help the community: - Make env-doctor more useful for more models - Build an accurate VRAM database - Enable better recommendations for all users</p> <p>Contributors are recognized in: - GitHub commit history - Pull request discussions - Model database comments (if notable measurements included)</p>"},{"location":"ADDING_MODELS/#future-enhancements","title":"Future Enhancements","text":"<p>Models you add today enable future features like: - Fine-tuning VRAM requirements - Batch size optimization recommendations - Context length impact on VRAM - Multi-GPU sharding strategies</p>"},{"location":"CONTAINER_BEST_PRACTICES/","title":"Container Best Practices for GPU Workloads","text":"<p>This guide explains best practices for running GPU workloads in Docker containers, and why the <code>env-doctor</code> validators flag certain patterns.</p>"},{"location":"CONTAINER_BEST_PRACTICES/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Base Images</li> <li>PyTorch Installation</li> <li>TensorFlow Installation</li> <li>Driver Management</li> <li>CUDA Toolkit</li> <li>Docker Compose Configuration</li> <li>Host System Requirements</li> </ul>"},{"location":"CONTAINER_BEST_PRACTICES/#base-images","title":"Base Images","text":""},{"location":"CONTAINER_BEST_PRACTICES/#use-gpu-enabled-base-images","title":"\u2705 Use GPU-Enabled Base Images","text":"<p>Always use a CUDA-enabled base image for GPU workloads.</p> <pre><code># \u2705 GOOD - NVIDIA CUDA base image\nFROM nvidia/cuda:12.1.0-runtime-ubuntu22.04\n\n# \u2705 GOOD - PyTorch official image\nFROM pytorch/pytorch:2.1.0-cuda12.1-cudnn8-runtime\n\n# \u2705 GOOD - TensorFlow GPU image\nFROM tensorflow/tensorflow:latest-gpu\n</code></pre> <p>Why? These images include the CUDA runtime libraries needed to communicate with the GPU driver.</p>"},{"location":"CONTAINER_BEST_PRACTICES/#dont-use-cpu-only-images","title":"\u274c Don't Use CPU-Only Images","text":"<pre><code># \u274c BAD - CPU-only image\nFROM python:3.10\n\n# \u274c BAD - Generic Ubuntu\nFROM ubuntu:22.04\n\n# \u274c BAD - Alpine (no CUDA support)\nFROM alpine:latest\n</code></pre> <p>Why? These images lack CUDA runtime libraries. Your GPU code will fail at runtime with errors like: - <code>CUDA not available</code> - <code>No CUDA-capable device detected</code> - <code>Could not load dynamic library 'libcudart.so'</code></p>"},{"location":"CONTAINER_BEST_PRACTICES/#runtime-vs-devel-images","title":"Runtime vs Devel Images","text":"<p>Choose the right image variant:</p> <ul> <li>Runtime (<code>-runtime</code>): For running pre-trained models. Smaller size (~2-3GB).</li> <li>Devel (<code>-devel</code>): For compiling CUDA code (flash-attention, xformers). Larger size (~6-8GB).</li> </ul> <pre><code># For inference only\nFROM nvidia/cuda:12.1.0-runtime-ubuntu22.04\n\n# For training with custom CUDA extensions\nFROM nvidia/cuda:12.1.0-devel-ubuntu22.04\n</code></pre>"},{"location":"CONTAINER_BEST_PRACTICES/#pytorch-installation","title":"PyTorch Installation","text":""},{"location":"CONTAINER_BEST_PRACTICES/#always-use-index-url","title":"\u2705 Always Use --index-url","text":"<p>Never install PyTorch without specifying the CUDA version.</p> <pre><code># \u2705 GOOD - Explicit CUDA version\nRUN pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121\n\n# \u2705 GOOD - CUDA 11.8\nRUN pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118\n</code></pre>"},{"location":"CONTAINER_BEST_PRACTICES/#dont-install-without-index-url","title":"\u274c Don't Install Without --index-url","text":"<pre><code># \u274c BAD - May download wrong CUDA version\nRUN pip install torch torchvision\n\n# \u274c BAD - Defaults to latest, may mismatch driver\nRUN pip install torch\n</code></pre> <p>Why? Without <code>--index-url</code>, pip defaults to the latest CUDA version (often 12.4+), which may not match: - Your base image's CUDA version - Your host driver's supported CUDA version</p> <p>This causes runtime errors like: - <code>CUDA driver version is insufficient</code> - <code>no kernel image is available for execution</code></p>"},{"location":"CONTAINER_BEST_PRACTICES/#match-base-image-cuda-version","title":"Match Base Image CUDA Version","text":"<p>If your base image is <code>nvidia/cuda:11.8</code>, use <code>cu118</code>:</p> <pre><code>FROM nvidia/cuda:11.8.0-runtime-ubuntu22.04\nRUN pip install torch --index-url https://download.pytorch.org/whl/cu118\n</code></pre>"},{"location":"CONTAINER_BEST_PRACTICES/#cuda-version-mapping","title":"CUDA Version Mapping","text":"Base Image CUDA --index-url suffix 11.7.x cu117 11.8.x cu118 12.0.x cu120 12.1.x cu121 12.2.x cu122"},{"location":"CONTAINER_BEST_PRACTICES/#tensorflow-installation","title":"TensorFlow Installation","text":""},{"location":"CONTAINER_BEST_PRACTICES/#use-tensorflowand-cuda","title":"\u2705 Use tensorflow[and-cuda]","text":"<pre><code># \u2705 GOOD - Explicit GPU support\nRUN pip install tensorflow[and-cuda]\n\n# \u2705 ALSO GOOD - Plain tensorflow with CUDA available\nRUN pip install tensorflow\n</code></pre>"},{"location":"CONTAINER_BEST_PRACTICES/#avoid-tensorflow-gpu","title":"\u26a0\ufe0f Avoid tensorflow-gpu","text":"<pre><code># \u26a0\ufe0f DEPRECATED - Old package name\nRUN pip install tensorflow-gpu\n</code></pre> <p>Why? <code>tensorflow-gpu</code> is deprecated. Modern TensorFlow (2.0+) automatically uses GPU when available. Use <code>tensorflow[and-cuda]</code> to ensure CUDA dependencies are installed.</p>"},{"location":"CONTAINER_BEST_PRACTICES/#driver-management","title":"Driver Management","text":""},{"location":"CONTAINER_BEST_PRACTICES/#never-install-nvidia-drivers-in-containers","title":"\u274c NEVER Install NVIDIA Drivers in Containers","text":"<pre><code># \u274c WRONG - Don't install drivers\nRUN apt-get install -y nvidia-driver-535\n\n# \u274c WRONG - Don't install any nvidia-driver-*\nRUN apt-get install -y nvidia-driver-*\n</code></pre> <p>Why? 1. Drivers must be on the host, not in containers 2. Container shares host's kernel and GPU driver 3. Installing drivers in container:    - Wastes space (~500MB)    - Can cause conflicts    - Won't work (container can't load kernel modules)</p>"},{"location":"CONTAINER_BEST_PRACTICES/#correct-driver-setup","title":"\u2705 Correct Driver Setup","text":"<ul> <li>Host: Install NVIDIA driver on host machine</li> <li>Container: Use CUDA-enabled base image (has runtime libraries)</li> <li>Docker: Install <code>nvidia-container-toolkit</code> on host</li> </ul> <pre><code># On host machine (Ubuntu/Debian)\nsudo apt-get install -y nvidia-driver-535\nsudo apt-get install -y nvidia-container-toolkit\nsudo systemctl restart docker\n</code></pre>"},{"location":"CONTAINER_BEST_PRACTICES/#cuda-toolkit","title":"CUDA Toolkit","text":""},{"location":"CONTAINER_BEST_PRACTICES/#toolkit-usually-not-needed","title":"\u26a0\ufe0f Toolkit Usually Not Needed","text":"<p>Most containers don't need the full CUDA Toolkit.</p> <pre><code># \u26a0\ufe0f USUALLY UNNECESSARY - Adds 2-5GB\nRUN apt-get install -y cuda-toolkit-12-1\n</code></pre> <p>Why? - Runtime-only containers have everything needed to run GPU code - Toolkit is only for compiling CUDA code - Adds significant image bloat</p>"},{"location":"CONTAINER_BEST_PRACTICES/#only-install-if-compiling","title":"\u2705 Only Install If Compiling","text":"<p>Install toolkit only when building from source:</p> <pre><code># \u2705 GOOD - Needed for flash-attention\nFROM nvidia/cuda:12.1.0-devel-ubuntu22.04\n\nRUN apt-get update &amp;&amp; apt-get install -y \\\n    cuda-toolkit-12-1 \\\n    build-essential\n\nRUN pip install flash-attn --no-build-isolation\n</code></pre> <p>When you need toolkit: - Building <code>flash-attention</code> - Building <code>xformers</code> - Compiling custom CUDA kernels - Building <code>auto-gptq</code> from source</p> <p>When you DON'T need it: - Installing PyTorch/TensorFlow wheels (pre-compiled) - Running inference - Running training with standard ops</p>"},{"location":"CONTAINER_BEST_PRACTICES/#docker-compose-configuration","title":"Docker Compose Configuration","text":""},{"location":"CONTAINER_BEST_PRACTICES/#modern-gpu-configuration-compose-v23","title":"\u2705 Modern GPU Configuration (Compose v2.3+)","text":"<pre><code>version: '3.8'\n\nservices:\n  gpu-app:\n    image: nvidia/cuda:12.1.0-runtime-ubuntu22.04\n    deploy:\n      resources:\n        reservations:\n          devices:\n            - driver: nvidia\n              count: all  # or 1, 2, etc.\n              capabilities: [gpu]\n    command: python train.py\n</code></pre> <p>Required fields: - <code>driver: nvidia</code> - Specifies NVIDIA GPU - <code>count: all</code> or specific number - How many GPUs - <code>capabilities: [gpu]</code> - Enable GPU capability</p>"},{"location":"CONTAINER_BEST_PRACTICES/#deprecated-syntax","title":"\u274c Deprecated Syntax","text":"<pre><code># \u274c OLD - Deprecated in Compose v2.3+\nservices:\n  old-app:\n    image: nvidia/cuda:12.1.0-runtime-ubuntu22.04\n    runtime: nvidia\n</code></pre> <p>Why deprecated? - Less flexible (can't control GPU allocation) - Removed in Compose v3+ - Doesn't support advanced features (device selection, memory limits)</p>"},{"location":"CONTAINER_BEST_PRACTICES/#multi-gpu-services","title":"Multi-GPU Services","text":"<p>Assign specific GPUs to different services:</p> <pre><code>version: '3.8'\n\nservices:\n  training:\n    deploy:\n      resources:\n        reservations:\n          devices:\n            - driver: nvidia\n              device_ids: ['0', '1']  # First two GPUs\n              capabilities: [gpu]\n\n  inference:\n    deploy:\n      resources:\n        reservations:\n          devices:\n            - driver: nvidia\n              device_ids: ['2']  # Third GPU\n              capabilities: [gpu]\n</code></pre>"},{"location":"CONTAINER_BEST_PRACTICES/#host-system-requirements","title":"Host System Requirements","text":""},{"location":"CONTAINER_BEST_PRACTICES/#prerequisites","title":"Prerequisites","text":"<p>Before running GPU containers, ensure your host has:</p> <ol> <li> <p>NVIDIA Driver <pre><code>nvidia-smi  # Should show driver version and GPUs\n</code></pre></p> </li> <li> <p>nvidia-container-toolkit <pre><code># Ubuntu/Debian\nsudo apt-get install -y nvidia-container-toolkit\n\n# RHEL/CentOS\nsudo yum install -y nvidia-container-toolkit\n</code></pre></p> </li> <li> <p>Docker Daemon Configuration</p> </li> </ol> <p>Create/edit <code>/etc/docker/daemon.json</code>:    <pre><code>{\n  \"runtimes\": {\n    \"nvidia\": {\n      \"path\": \"nvidia-container-runtime\",\n      \"runtimeArgs\": []\n    }\n  }\n}\n</code></pre></p> <p>Restart Docker:    <pre><code>sudo systemctl restart docker\n</code></pre></p>"},{"location":"CONTAINER_BEST_PRACTICES/#verify-setup","title":"Verify Setup","text":"<p>Test GPU access in a container:</p> <pre><code>docker run --rm --gpus all nvidia/cuda:12.1.0-runtime-ubuntu22.04 nvidia-smi\n</code></pre> <p>Should display GPU information.</p>"},{"location":"CONTAINER_BEST_PRACTICES/#common-issues-and-solutions","title":"Common Issues and Solutions","text":""},{"location":"CONTAINER_BEST_PRACTICES/#issue-cuda-not-available-in-container","title":"Issue: \"CUDA not available\" in container","text":"<p>Causes: - Using CPU-only base image - nvidia-container-toolkit not installed - Missing <code>--gpus all</code> flag (when running <code>docker run</code>) - Missing GPU device config (in docker-compose)</p> <p>Solutions: 1. Use CUDA-enabled base image 2. Install nvidia-container-toolkit on host 3. Add GPU configuration to docker-compose.yml</p>"},{"location":"CONTAINER_BEST_PRACTICES/#issue-cuda-driver-version-insufficient","title":"Issue: \"CUDA driver version insufficient\"","text":"<p>Causes: - PyTorch/TensorFlow built for newer CUDA than driver supports - Missing <code>--index-url</code> in pip install</p> <p>Solution: <pre><code># Check driver's max CUDA version\nnvidia-smi | grep \"CUDA Version\"\n\n# Install matching PyTorch\n# If driver supports CUDA 11.8:\npip install torch --index-url https://download.pytorch.org/whl/cu118\n</code></pre></p>"},{"location":"CONTAINER_BEST_PRACTICES/#issue-image-too-large","title":"Issue: Image too large","text":"<p>Causes: - Using <code>-devel</code> images when <code>-runtime</code> is sufficient - Installing CUDA toolkit unnecessarily</p> <p>Solutions: 1. Use <code>-runtime</code> images for inference 2. Only install toolkit when compiling CUDA code 3. Use multi-stage builds to copy only needed artifacts</p>"},{"location":"CONTAINER_BEST_PRACTICES/#quick-validation","title":"Quick Validation","text":"<p>Before building your container, run:</p> <pre><code># Validate Dockerfile\nenv-doctor dockerfile\n\n# Validate docker-compose.yml\nenv-doctor docker-compose\n</code></pre> <p>Fix all errors before building to avoid runtime issues.</p>"},{"location":"CONTAINER_BEST_PRACTICES/#additional-resources","title":"Additional Resources","text":"<ul> <li>NVIDIA Container Toolkit Documentation</li> <li>Docker Compose GPU Access</li> <li>PyTorch Docker Images</li> <li>TensorFlow Docker Images</li> </ul>"},{"location":"QUICK_START_MCP/","title":"Quick Start: MCP Testing","text":""},{"location":"QUICK_START_MCP/#run-all-tests-fast","title":"Run All Tests (Fast)","text":"<pre><code>python tests/test_mcp_tools.py\n</code></pre>"},{"location":"QUICK_START_MCP/#interactive-testing","title":"Interactive Testing","text":"<pre><code>python tests/test_mcp_interactive.py\n</code></pre>"},{"location":"QUICK_START_MCP/#test-specific-tool","title":"Test Specific Tool","text":"<pre><code># CUDA installation guide\npython tests/test_mcp_interactive.py cuda_install\n\n# Get PyTorch install command\npython tests/test_mcp_interactive.py install_command '{\"library\":\"torch\"}'\n\n# Check if model fits\npython tests/test_mcp_interactive.py model_check '{\"model_name\":\"llama-3-8b\"}'\n\n# Full environment check\npython tests/test_mcp_interactive.py env_check\n\n# Check Python version compatibility with AI libraries\npython tests/test_mcp_interactive.py python_compat_check\n\n# Detailed CUDA info\npython tests/test_mcp_interactive.py cuda_info\n</code></pre>"},{"location":"QUICK_START_MCP/#all-11-tools","title":"All 11 Tools","text":"Tool Quick Test Command <code>env_check</code> <code>python tests/test_mcp_interactive.py env_check</code> <code>env_check_component</code> <code>python tests/test_mcp_interactive.py env_check_component '{\"component\":\"nvidia_driver\"}'</code> <code>python_compat_check</code> <code>python tests/test_mcp_interactive.py python_compat_check</code> <code>cuda_info</code> <code>python tests/test_mcp_interactive.py cuda_info</code> <code>cudnn_info</code> <code>python tests/test_mcp_interactive.py cudnn_info</code> <code>cuda_install</code> <code>python tests/test_mcp_interactive.py cuda_install</code> <code>install_command</code> <code>python tests/test_mcp_interactive.py install_command '{\"library\":\"torch\"}'</code> <code>model_check</code> <code>python tests/test_mcp_interactive.py model_check '{\"model_name\":\"llama-3-8b\"}'</code> <code>model_list</code> <code>python tests/test_mcp_interactive.py model_list</code> <code>dockerfile_validate</code> (needs content string) <code>docker_compose_validate</code> (needs content string)"},{"location":"QUICK_START_MCP/#example-session","title":"Example Session","text":"<pre><code># 1. Check what's installed\npython tests/test_mcp_interactive.py env_check\n\n# 2. Get CUDA installation instructions\npython tests/test_mcp_interactive.py cuda_install\n\n# 3. Get PyTorch install command\npython tests/test_mcp_interactive.py install_command '{\"library\":\"torch\"}'\n\n# 4. Check if your target model will fit\npython tests/test_mcp_interactive.py model_check '{\"model_name\":\"llama-3-70b\"}'\n</code></pre>"},{"location":"QUICK_START_MCP/#output-format","title":"Output Format","text":"<p>All tools return JSON with consistent structure: <pre><code>{\n  \"success\": true/false,\n  \"detected\": true/false,\n  \"version\": \"...\",\n  \"status\": \"success/warning/error\",\n  \"issues\": [...],\n  \"recommendations\": [...],\n  \"error\": \"...\" (if failed)\n}\n</code></pre></p>"},{"location":"QUICK_START_MCP/#common-use-cases","title":"Common Use Cases","text":""},{"location":"QUICK_START_MCP/#setup-new-machine","title":"Setup New Machine","text":"<ol> <li><code>env_check</code> - see what's missing</li> <li><code>cuda_install</code> - get installation steps</li> <li><code>install_command</code> - get library commands</li> </ol>"},{"location":"QUICK_START_MCP/#debug-issues","title":"Debug Issues","text":"<ol> <li><code>python_compat_check</code> - check Python version conflicts</li> <li><code>cuda_info</code> - detailed CUDA analysis</li> <li><code>cudnn_info</code> - detailed cuDNN analysis</li> <li><code>env_check</code> - overall status</li> </ol>"},{"location":"QUICK_START_MCP/#deploy-model","title":"Deploy Model","text":"<ol> <li><code>model_check</code> - will it fit?</li> <li><code>dockerfile_validate</code> - validate config</li> <li><code>docker_compose_validate</code> - validate deployment</li> </ol>"},{"location":"QUICK_START_MCP/#see-full-documentation","title":"See Full Documentation","text":"<p>Read <code>MCP_TESTING.md</code> for: - Detailed examples for each tool - JSON-RPC protocol details - Integration with Claude Desktop - Troubleshooting guide</p>"},{"location":"architecture/","title":"Architecture","text":"<p>Understanding how Env-Doctor works internally.</p>"},{"location":"architecture/#overview","title":"Overview","text":"<pre><code>flowchart TB\n    CLI[\"CLI&lt;br/&gt;(cli.py)\"]\n    REG[\"Detector Registry&lt;br/&gt;(Discovers &amp; runs detectors)\"]\n\n    CLI --&gt; REG\n\n    REG --&gt; WSL[\"WSL2&lt;br/&gt;Detector\"]\n    REG --&gt; DRV[\"Driver&lt;br/&gt;Detector\"]\n    REG --&gt; CUDA[\"CUDA&lt;br/&gt;Detector\"]\n    REG --&gt; PY[\"Python&lt;br/&gt;Detector\"]\n\n    REG --&gt; DB[\"Compatibility Database&lt;br/&gt;(compatibility.json)\"]\n</code></pre>"},{"location":"architecture/#components","title":"Components","text":""},{"location":"architecture/#the-cli-clipy","title":"The CLI (<code>cli.py</code>)","text":"<p>Entry point for all commands. Orchestrates detectors and presents unified diagnostics.</p>"},{"location":"architecture/#the-detector-registry","title":"The Detector Registry","text":"<p>Plugin system for detector discovery and execution. Allows easy addition of new detectors.</p> <pre><code>from env_doctor.registry import DetectorRegistry\n\nregistry = DetectorRegistry()\nresults = registry.run_all()\n</code></pre>"},{"location":"architecture/#detectors","title":"Detectors","text":"<p>Modular detection system with specialized detectors:</p> Detector Purpose <code>WSL2Detector</code> Environment detection, GPU forwarding validation <code>NvidiaDriverDetector</code> GPU driver version and capability detection <code>CudaToolkitDetector</code> System CUDA installation detection <code>CudnnDetector</code> cuDNN library detection and validation <code>PythonLibraryDetector</code> Python AI library version and CUDA compatibility <p>Each detector returns a <code>DetectorResult</code> with:</p> <ul> <li><code>status</code>: SUCCESS, WARNING, ERROR, NOT_FOUND</li> <li><code>component</code>: What was detected</li> <li><code>version</code>: Version string</li> <li><code>path</code>: Installation path (if applicable)</li> <li><code>metadata</code>: Additional detection details</li> </ul>"},{"location":"architecture/#the-compatibility-database-compatibilityjson","title":"The Compatibility Database (<code>compatibility.json</code>)","text":"<p>Maps drivers to maximum supported CUDA versions and verified wheel URLs:</p> <pre><code>{\n  \"cuda_driver_mapping\": {\n    \"535.104.05\": \"12.2\",\n    \"525.85.12\": \"12.0\",\n    \"470.82.01\": \"11.4\"\n  },\n  \"pytorch_wheels\": {\n    \"cu121\": \"https://download.pytorch.org/whl/cu121\",\n    \"cu118\": \"https://download.pytorch.org/whl/cu118\"\n  }\n}\n</code></pre>"},{"location":"architecture/#the-updater-dbpy","title":"The Updater (<code>db.py</code>)","text":"<p>Fetches the latest compatibility rules from GitHub so you don't need to update the package frequently.</p>"},{"location":"architecture/#automated-database-updates","title":"Automated Database Updates","text":"<p>The compatibility database is maintained through automation:</p>"},{"location":"architecture/#scraper-toolsscraperpy-batch-job-currently-paused","title":"Scraper (<code>tools/scraper.py</code>) (Batch Job currently paused)","text":"<ul> <li>GitHub Actions runs periodically</li> <li>Scrapes official PyTorch/TensorFlow/JAX documentation</li> <li>Extracts CUDA compatibility mappings</li> <li>Creates pull requests with updates</li> </ul>"},{"location":"architecture/#validator-toolsvalidatorpy-not-implemented-yet","title":"Validator (<code>tools/validator.py</code>) (Not Implemented yet)","text":"<ul> <li>Validates scraped data structure</li> <li>Ensures version strings are parseable</li> <li>Catches malformed entries</li> </ul>"},{"location":"architecture/#human-review-currently-prioritised-as-main-source","title":"Human Review (Currently prioritised as main source)","text":"<ul> <li>Automated updates create PRs (not auto-merged)</li> <li>Maintainers review before merging</li> <li>Community can flag issues</li> </ul>"},{"location":"architecture/#adding-a-new-detector","title":"Adding a New Detector","text":"<ol> <li>Create a detector class:</li> </ol> <pre><code>from env_doctor.detectors.base import BaseDetector, DetectorResult, Status\n\nclass MyDetector(BaseDetector):\n    name = \"my_component\"\n\n    def detect(self) -&gt; DetectorResult:\n        # Detection logic here\n        return DetectorResult(\n            status=Status.SUCCESS,\n            component=self.name,\n            version=\"1.0.0\",\n            metadata={\"key\": \"value\"}\n        )\n</code></pre> <ol> <li> <p>Register in the registry (automatic via naming convention)</p> </li> <li> <p>Add to CLI if needed</p> </li> </ol>"},{"location":"architecture/#see-also","title":"See Also","text":"<ul> <li>Contributing Guide</li> <li>debug Command - View raw detector output</li> </ul>"},{"location":"getting-started/","title":"Getting Started","text":"<p>This guide will help you install Env-Doctor and run your first environment check.</p>"},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.7 or higher</li> <li>NVIDIA GPU (optional, but recommended)</li> <li>NVIDIA driver installed (for GPU features)</li> </ul>"},{"location":"getting-started/#installation","title":"Installation","text":"<pre><code>pip install env-doctor\n</code></pre>"},{"location":"getting-started/#your-first-check","title":"Your First Check","text":"<p>Run the environment check to see your current GPU/CUDA status:</p> <pre><code>env-doctor check\n</code></pre> <p>This command checks:</p> <ul> <li>Environment: Native Linux vs WSL1 vs WSL2, with GPU forwarding validation</li> <li>GPU Driver: Version and maximum supported CUDA version</li> <li>System CUDA: Installation status and version</li> <li>Library Conflicts: Detects mismatched versions (e.g., PyTorch built for CUDA 12.4 on a driver supporting only CUDA 11.8)</li> <li>WSL2 GPU Setup: Validates CUDA libraries and driver configuration</li> </ul>"},{"location":"getting-started/#example-output","title":"Example Output","text":"<pre><code>\ud83e\ude7a ENV-DOCTOR DIAGNOSIS\n============================================================\n\n\ud83d\udda5\ufe0f  Environment: Native Linux\n\n\ud83c\udfae GPU Driver\n   \u2705 NVIDIA Driver: 535.146.02\n   \u2514\u2500 Max CUDA: 12.2\n\n\ud83d\udd27 CUDA Toolkit\n   \u2705 System CUDA: 12.1.1\n   \u2514\u2500 Path: /usr/local/cuda-12.1\n\n\ud83d\udce6 Python Libraries\n   \u2705 torch 2.1.0+cu121\n   \u2705 tensorflow 2.15.0\n\n\u2705 All checks passed! Your environment is healthy.\n</code></pre>"},{"location":"getting-started/#common-next-steps","title":"Common Next Steps","text":""},{"location":"getting-started/#get-cuda-toolkit-installation-instructions","title":"Get CUDA Toolkit Installation Instructions","text":"<p>If you need to install CUDA Toolkit on your system:</p> <pre><code>env-doctor cuda-install\n</code></pre> <p>This provides platform-specific, copy-paste installation commands for your system (Ubuntu, Debian, RHEL, Fedora, WSL2, Windows, Conda).</p>"},{"location":"getting-started/#get-a-safe-install-command","title":"Get a Safe Install Command","text":"<p>If you need to install or reinstall PyTorch:</p> <pre><code>env-doctor install torch\n</code></pre> <p>This outputs the exact <code>pip install</code> command with the correct <code>--index-url</code> for your driver.</p>"},{"location":"getting-started/#check-model-compatibility","title":"Check Model Compatibility","text":"<p>Before downloading a large model:</p> <pre><code>env-doctor model llama-3-8b\n</code></pre>"},{"location":"getting-started/#validate-your-dockerfile","title":"Validate Your Dockerfile","text":"<p>Before building a GPU container:</p> <pre><code>env-doctor dockerfile path/to/Dockerfile\n</code></pre>"},{"location":"getting-started/#quick-command-reference","title":"Quick Command Reference","text":"Command Purpose <code>env-doctor check</code> Full environment diagnosis <code>env-doctor cuda-install</code> Get CUDA Toolkit installation guide <code>env-doctor install &lt;lib&gt;</code> Get safe install command <code>env-doctor model &lt;name&gt;</code> Check model VRAM requirements <code>env-doctor cuda-info</code> Detailed CUDA toolkit analysis <code>env-doctor cudnn-info</code> cuDNN library analysis <code>env-doctor dockerfile</code> Validate Dockerfile <code>env-doctor docker-compose</code> Validate docker-compose.yml <code>env-doctor scan</code> Scan for deprecated imports <code>env-doctor debug</code> Verbose detector output"},{"location":"getting-started/#next-steps","title":"Next Steps","text":"<ul> <li>Command Reference - Detailed documentation for each command</li> <li>WSL2 Guide - GPU setup for Windows Subsystem for Linux</li> <li>CI/CD Integration - Use Env-Doctor in your pipelines</li> </ul>"},{"location":"commands/check/","title":"check","text":"<p>Diagnose your environment for GPU/CUDA compatibility issues.</p>"},{"location":"commands/check/#usage","title":"Usage","text":"<pre><code>env-doctor check\n</code></pre>"},{"location":"commands/check/#what-it-checks","title":"What It Checks","text":""},{"location":"commands/check/#environment-detection","title":"Environment Detection","text":"<ul> <li>Native Linux: Standard Linux environment</li> <li>WSL1: Detects WSL1 and warns that CUDA is not supported</li> <li>WSL2: Full GPU forwarding validation</li> </ul>"},{"location":"commands/check/#gpu-driver","title":"GPU Driver","text":"<ul> <li>Driver version detection via NVML</li> <li>Maximum supported CUDA version</li> <li>Driver health status</li> </ul>"},{"location":"commands/check/#system-cuda-toolkit","title":"System CUDA Toolkit","text":"<ul> <li>Installation path and version</li> <li>Multiple installation detection</li> <li>PATH and environment configuration</li> </ul>"},{"location":"commands/check/#python-libraries","title":"Python Libraries","text":"<ul> <li>PyTorch, TensorFlow, JAX detection</li> <li>CUDA version each library was compiled for</li> <li>Compatibility with your driver</li> </ul>"},{"location":"commands/check/#gpu-compute-capability","title":"GPU Compute Capability","text":"<p>Checks whether the installed PyTorch wheel includes compiled kernels for your GPU's SM architecture. This catches a silent failure mode common with new GPU generations: everything looks healthy (<code>nvidia-smi</code>, <code>nvcc</code>, driver all pass) but CUDA may not work correctly because the stable PyTorch wheel doesn't include kernels for the new architecture.</p> <p>env-doctor probes <code>torch.cuda.is_available()</code> at runtime and distinguishes two failure modes:</p> <ul> <li>Hard failure \u2014 <code>is_available()</code> returns <code>False</code>. The GPU cannot be used at all.</li> <li>Soft failure \u2014 <code>is_available()</code> returns <code>True</code> via NVIDIA's driver-level PTX JIT, but complex CUDA ops may silently degrade or fail.</li> </ul> <p>Other behaviours: - Reads GPU compute capability from the driver (e.g. <code>12.0</code> for Blackwell RTX 5070) - Reads the compiled SM list from <code>torch.cuda.get_arch_list()</code> - Handles PTX forward compatibility \u2014 <code>compute_90</code> in the arch list covers newer SMs via JIT compilation - On mismatch, prints the exact nightly install command to fix it</p>"},{"location":"commands/check/#library-conflicts","title":"Library Conflicts","text":"<p>Detects \"Frankenstein\" environments where:</p> <ul> <li>PyTorch is built for CUDA 12.4 but driver only supports 11.8</li> <li>Multiple libraries compiled for different CUDA versions</li> <li>System toolkit doesn't match library requirements</li> </ul>"},{"location":"commands/check/#example-output","title":"Example Output","text":"<pre><code>\ud83e\ude7a ENV-DOCTOR DIAGNOSIS\n============================================================\n\n\ud83d\udda5\ufe0f  Environment: WSL2 (GPU forwarding enabled)\n\n\ud83c\udfae GPU Driver\n   \u2705 NVIDIA Driver: 535.146.02\n   \u2514\u2500 Max CUDA: 12.2\n\n\ud83d\udd27 CUDA Toolkit\n   \u2705 System CUDA: 12.1.1\n   \u2514\u2500 Path: /usr/local/cuda-12.1\n\n\ud83d\udce6 Python Libraries\n   \u2705 torch 2.1.0+cu121\n   \u2514\u2500 CUDA 12.1 \u2713 (compatible with driver)\n\n\u2705 All checks passed!\n</code></pre>"},{"location":"commands/check/#compute-capability-compatible","title":"Compute Capability: Compatible","text":"<pre><code>\ud83c\udfaf  COMPUTE CAPABILITY CHECK\n    GPU: NVIDIA GeForce GTX 1650 (Compute 7.5, Turing, sm_75)\n    PyTorch compiled for: sm_50, sm_60, sm_61, sm_70, sm_75, sm_80, sm_86, sm_90\n    \u2705 COMPATIBLE: PyTorch 2.5.1+cu121 supports your GPU architecture.\n</code></pre>"},{"location":"commands/check/#compute-capability-hard-mismatch-is_available-false","title":"Compute Capability: Hard Mismatch (<code>is_available()</code> \u2192 <code>False</code>)","text":"<pre><code>\ud83c\udfaf  COMPUTE CAPABILITY CHECK\n    GPU: NVIDIA GeForce RTX 5070 (Compute 12.0, Blackwell, sm_120)\n    PyTorch compiled for: sm_50, sm_60, sm_70, sm_80, sm_90, compute_90\n    \u274c ARCHITECTURE MISMATCH: Your GPU needs sm_120 but PyTorch 2.5.1 doesn't include it.\n\n    This is likely why torch.cuda.is_available() returns False even though\n    your driver and CUDA toolkit are working correctly.\n\n    FIX: Install PyTorch nightly with sm_120 support:\n       pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu126\n</code></pre>"},{"location":"commands/check/#compute-capability-soft-mismatch-is_available-true-via-ptx-jit","title":"Compute Capability: Soft Mismatch (<code>is_available()</code> \u2192 <code>True</code> via PTX JIT)","text":"<pre><code>\ud83c\udfaf  COMPUTE CAPABILITY CHECK\n    GPU: NVIDIA GeForce RTX 5070 (Compute 12.0, Blackwell, sm_120)\n    PyTorch compiled for: sm_50, sm_60, sm_70, sm_80, sm_90, compute_90\n    \u26a0\ufe0f  ARCHITECTURE MISMATCH (Soft): Your GPU needs sm_120 but PyTorch 2.5.1 doesn't include it.\n\n    torch.cuda.is_available() returned True via NVIDIA's driver-level PTX JIT,\n    but you may experience degraded performance or failures with complex CUDA ops.\n\n    FIX: Install a newer PyTorch with native sm_120 support for full compatibility:\n       pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu126\n</code></pre>"},{"location":"commands/check/#common-issues-detected","title":"Common Issues Detected","text":""},{"location":"commands/check/#driver-too-old","title":"Driver Too Old","text":"<pre><code>\u274c PyTorch requires CUDA 12.1 but driver only supports CUDA 11.8\n   \u2192 Update your NVIDIA driver to 520.61.05 or newer\n   \u2192 Or install PyTorch for CUDA 11.8:\n     pip install torch --index-url https://download.pytorch.org/whl/cu118\n</code></pre>"},{"location":"commands/check/#missing-cuda-toolkit","title":"Missing CUDA Toolkit","text":"<pre><code>\u26a0\ufe0f  No system CUDA toolkit found\n   \u2192 This is OK if you only use PyTorch/TensorFlow (they bundle CUDA)\n   \u2192 Install CUDA toolkit if you need to compile extensions\n</code></pre>"},{"location":"commands/check/#wsl2-gpu-issues","title":"WSL2 GPU Issues","text":"<pre><code>\u274c NVIDIA driver installed inside WSL. This breaks GPU forwarding.\n   \u2192 Run: sudo apt remove --purge nvidia-*\n</code></pre>"},{"location":"commands/check/#advanced-options","title":"Advanced Options","text":""},{"location":"commands/check/#json-output","title":"JSON Output","text":"<p>For scripting or parsing results programmatically:</p> <pre><code>env-doctor check --json\n</code></pre> <pre><code>{\n  \"status\": \"success\",\n  \"timestamp\": \"2026-01-15T10:30:00Z\",\n  \"summary\": {\n    \"driver\": \"found\",\n    \"cuda\": \"found\",\n    \"issues_count\": 0\n  },\n  \"checks\": {\n    \"driver\": {\n      \"component\": \"nvidia_driver\",\n      \"status\": \"success\",\n      \"detected\": true,\n      \"version\": \"535.146.02\",\n      \"metadata\": {\n        \"max_cuda_version\": \"12.2\"\n      }\n    },\n    \"cuda\": {\n      \"component\": \"cuda_toolkit\",\n      \"status\": \"success\",\n      \"detected\": true,\n      \"version\": \"12.1.1\",\n      \"path\": \"/usr/local/cuda-12.1\"\n    },\n    \"libraries\": {\n      \"torch\": {\n        \"version\": \"2.1.0+cu121\",\n        \"cuda_version\": \"12.1\",\n        \"compatible\": true\n      }\n    },\n    \"compute_compatibility\": {\n      \"gpu_name\": \"NVIDIA GeForce GTX 1650\",\n      \"compute_capability\": \"7.5\",\n      \"sm\": \"sm_75\",\n      \"arch_name\": \"Turing\",\n      \"arch_list\": [\"sm_50\", \"sm_60\", \"sm_70\", \"sm_75\", \"sm_80\", \"sm_86\", \"sm_90\"],\n      \"status\": \"compatible\",\n      \"cuda_available\": true,\n      \"message\": \"PyTorch supports sm_75 (Turing)\"\n    }\n  }\n}\n</code></pre>"},{"location":"commands/check/#cicd-mode","title":"CI/CD Mode","text":"<p>For continuous integration pipelines:</p> <pre><code>env-doctor check --ci\n</code></pre> <p>This implies <code>--json</code> and sets proper exit codes:</p> Code Meaning <code>0</code> All checks passed <code>1</code> Warnings or non-critical issues <code>2</code> Critical errors detected <p>See CI/CD Integration Guide for full pipeline examples.</p>"},{"location":"commands/check/#see-also","title":"See Also","text":"<ul> <li>cuda-info - Detailed CUDA toolkit analysis</li> <li>cudnn-info - cuDNN library analysis</li> <li>debug - Verbose detector output</li> </ul>"},{"location":"commands/cuda-info/","title":"cuda-info","text":"<p>Get detailed information about your CUDA toolkit installations.</p>"},{"location":"commands/cuda-info/#usage","title":"Usage","text":"<pre><code>env-doctor cuda-info\n</code></pre>"},{"location":"commands/cuda-info/#what-it-shows","title":"What It Shows","text":""},{"location":"commands/cuda-info/#cuda-installations","title":"CUDA Installations","text":"<ul> <li>All CUDA toolkit installations found on your system</li> <li>Version of each installation</li> <li>Installation paths</li> </ul>"},{"location":"commands/cuda-info/#environment-variables","title":"Environment Variables","text":"<ul> <li><code>CUDA_HOME</code> configuration</li> <li><code>PATH</code> entries for CUDA binaries</li> <li><code>LD_LIBRARY_PATH</code> (Linux) or <code>PATH</code> (Windows) for libraries</li> </ul>"},{"location":"commands/cuda-info/#runtime-libraries","title":"Runtime Libraries","text":"<ul> <li><code>libcudart</code> (CUDA runtime) status</li> <li>Library paths and versions</li> </ul>"},{"location":"commands/cuda-info/#compatibility","title":"Compatibility","text":"<ul> <li>Driver compatibility with installed toolkit(s)</li> <li>Potential version conflicts</li> </ul>"},{"location":"commands/cuda-info/#example-output","title":"Example Output","text":"<pre><code>\ud83d\udd27 CUDA TOOLKIT ANALYSIS\n============================================================\n\n\ud83d\udccd Installations Found:\n\n  CUDA 12.1.1\n  \u2514\u2500 Path: /usr/local/cuda-12.1\n  \u2514\u2500 nvcc: /usr/local/cuda-12.1/bin/nvcc\n\n  CUDA 11.8.0\n  \u2514\u2500 Path: /usr/local/cuda-11.8\n  \u2514\u2500 nvcc: /usr/local/cuda-11.8/bin/nvcc\n\n\ud83c\udf10 Environment Configuration:\n\n  CUDA_HOME: /usr/local/cuda-12.1\n  PATH:\n    \u2705 /usr/local/cuda-12.1/bin\n  LD_LIBRARY_PATH:\n    \u2705 /usr/local/cuda-12.1/lib64\n\n\ud83d\udcda Runtime Libraries:\n\n  libcudart.so.12.1.105\n  \u2514\u2500 Path: /usr/local/cuda-12.1/lib64/libcudart.so.12\n\n\ud83c\udfae Driver Compatibility:\n\n  Driver: 535.146.02 (supports up to CUDA 12.2)\n  \u2705 CUDA 12.1 is compatible with your driver\n</code></pre>"},{"location":"commands/cuda-info/#multiple-installations","title":"Multiple Installations","text":"<p>When multiple CUDA versions are installed:</p> <pre><code>\u26a0\ufe0f  Multiple CUDA installations detected\n\n  Active: CUDA 12.1 (via CUDA_HOME)\n  Also found: CUDA 11.8\n\n\ud83d\udca1 Recommendations:\n  - Ensure CUDA_HOME points to the version you need\n  - Check PATH order if using nvcc\n</code></pre>"},{"location":"commands/cuda-info/#common-issues","title":"Common Issues","text":""},{"location":"commands/cuda-info/#cuda_home-not-set","title":"CUDA_HOME Not Set","text":"<pre><code>\u274c CUDA_HOME is not set\n\n\ud83d\udca1 Fix:\n  export CUDA_HOME=/usr/local/cuda-12.1\n  export PATH=$CUDA_HOME/bin:$PATH\n</code></pre>"},{"location":"commands/cuda-info/#path-mismatch","title":"PATH Mismatch","text":"<pre><code>\u26a0\ufe0f  PATH contains different CUDA version than CUDA_HOME\n\n  CUDA_HOME: /usr/local/cuda-12.1\n  PATH nvcc: /usr/local/cuda-11.8/bin/nvcc\n\n\ud83d\udca1 Fix:\n  Ensure PATH includes $CUDA_HOME/bin before other CUDA paths\n</code></pre>"},{"location":"commands/cuda-info/#advanced-options","title":"Advanced Options","text":""},{"location":"commands/cuda-info/#json-output","title":"JSON Output","text":"<pre><code>env-doctor cuda-info --json\n</code></pre>"},{"location":"commands/cuda-info/#see-also","title":"See Also","text":"<ul> <li>cudnn-info - cuDNN library analysis</li> <li>check - Full environment diagnosis</li> <li>debug - Verbose detector output</li> </ul>"},{"location":"commands/cuda-install/","title":"cuda-install","text":"<p>Get step-by-step CUDA Toolkit installation instructions tailored to your platform.</p>"},{"location":"commands/cuda-install/#overview","title":"Overview","text":"<p>The <code>cuda-install</code> command automatically:</p> <ol> <li>Detects your platform (OS, distribution, version, architecture, WSL2)</li> <li>Analyzes your GPU driver to determine the best CUDA version</li> <li>Provides copy-paste installation commands specific to your system</li> <li>Includes post-installation setup (environment variables, verification)</li> <li>Shows official download links for manual installation if needed</li> </ol>"},{"location":"commands/cuda-install/#usage","title":"Usage","text":""},{"location":"commands/cuda-install/#auto-detect-from-gpu-driver-recommended","title":"Auto-detect from GPU driver (recommended)","text":"<pre><code>env-doctor cuda-install\n</code></pre> <p>This will: - Check your NVIDIA driver version - Recommend the best CUDA Toolkit version - Show platform-specific installation steps</p>"},{"location":"commands/cuda-install/#install-specific-cuda-version","title":"Install specific CUDA version","text":"<pre><code>env-doctor cuda-install 12.4\nenv-doctor cuda-install 12.1\nenv-doctor cuda-install 11.8\n</code></pre>"},{"location":"commands/cuda-install/#example-output","title":"Example Output","text":""},{"location":"commands/cuda-install/#linux-ubuntu","title":"Linux (Ubuntu)","text":"<pre><code>$ env-doctor cuda-install\n</code></pre> <pre><code>============================================================\nCUDA TOOLKIT INSTALLATION GUIDE\n============================================================\n\nDetected Platform:\n    Linux (ubuntu 22.04, x86_64)\n\nDriver: 535.146.02 (supports up to CUDA 12.2)\nRecommended CUDA Toolkit: 12.1\n\n============================================================\nUbuntu 22.04 (x86_64) - Network Install\n============================================================\n\nInstallation Steps:\n------------------------------------------------------------\n    1. wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-keyring_1.1-1_all.deb\n    2. sudo dpkg -i cuda-keyring_1.1-1_all.deb\n    3. sudo apt-get update\n    4. sudo apt-get -y install cuda-toolkit-12-1\n\nPost-Installation Setup:\n------------------------------------------------------------\n    export PATH=/usr/local/cuda-12.1/bin${PATH:+:${PATH}}\n    export LD_LIBRARY_PATH=/usr/local/cuda-12.1/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}\n\n    TIP: Add the above exports to ~/.bashrc or ~/.zshrc\n\nVerify Installation:\n------------------------------------------------------------\n    nvcc --version\n\nOfficial Download Page:\n    https://developer.nvidia.com/cuda-12-1-0-download-archive\n\n============================================================\nAfter installation, run 'env-doctor check' to verify.\n============================================================\n</code></pre>"},{"location":"commands/cuda-install/#wsl2-special-instructions","title":"WSL2 (Special Instructions)","text":"<pre><code>$ env-doctor cuda-install\n</code></pre> <pre><code>============================================================\nCUDA TOOLKIT INSTALLATION GUIDE\n============================================================\n\nDetected Platform:\n    WSL2 (ubuntu 22.04)\n\nDriver: 560.35.03 (supports up to CUDA 12.6)\nRecommended CUDA Toolkit: 12.6\n\n============================================================\nWSL2 (Ubuntu) - DO NOT install driver inside WSL\n============================================================\n\nPrerequisites:\n    - Ensure Windows NVIDIA driver &gt;= 560.xx is installed on the HOST\n    - DO NOT install nvidia-driver packages inside WSL2\n\nInstallation Steps:\n------------------------------------------------------------\n    1. wget https://developer.download.nvidia.com/compute/cuda/repos/wsl-ubuntu/x86_64/cuda-keyring_1.1-1_all.deb\n    2. sudo dpkg -i cuda-keyring_1.1-1_all.deb\n    3. sudo apt-get update\n    4. sudo apt-get -y install cuda-toolkit-12-6\n\nPost-Installation Setup:\n------------------------------------------------------------\n    export PATH=/usr/local/cuda-12.6/bin${PATH:+:${PATH}}\n    export LD_LIBRARY_PATH=/usr/local/cuda-12.6/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}\n\n    TIP: Add the above exports to ~/.bashrc or ~/.zshrc\n\nVerify Installation:\n------------------------------------------------------------\n    nvcc --version\n\nNotes:\n    GPU driver is forwarded from Windows host. Only install the toolkit inside WSL2.\n\n============================================================\n</code></pre>"},{"location":"commands/cuda-install/#windows","title":"Windows","text":"<pre><code>$ env-doctor cuda-install\n</code></pre> <pre><code>============================================================\nCUDA TOOLKIT INSTALLATION GUIDE\n============================================================\n\nDetected Platform:\n    Windows (x86_64)\n\nDriver: 560.35.03 (supports up to CUDA 12.6)\nRecommended CUDA Toolkit: 12.6\n\n============================================================\nWindows 10/11 (x86_64) - GUI Installer\n============================================================\n\nInstallation Steps:\n------------------------------------------------------------\n    1. Download installer from: https://developer.nvidia.com/cuda-12-6-0-download-archive\n    2. Select: Windows &gt; x86_64 &gt; 10/11 &gt; exe (network)\n    3. Run the downloaded installer as Administrator\n    4. Select 'Custom Install' and check 'CUDA Toolkit'\n    5. Follow the installation wizard\n\nPost-Installation Setup:\n------------------------------------------------------------\n    The installer automatically adds CUDA to PATH\n    Verify: Open a NEW command prompt and run 'nvcc --version'\n\nVerify Installation:\n------------------------------------------------------------\n    nvcc --version\n\nNotes:\n    Restart your terminal/IDE after installation for PATH changes to take effect.\n\n============================================================\n</code></pre>"},{"location":"commands/cuda-install/#supported-platforms","title":"Supported Platforms","text":""},{"location":"commands/cuda-install/#linux-distributions","title":"Linux Distributions","text":"Distribution Versions Package Manager Ubuntu 18.04, 20.04, 22.04, 24.04 apt (deb) Debian 11, 12 apt (deb) RHEL / Rocky / AlmaLinux 7, 8, 9 dnf/yum (rpm) Fedora 39+ dnf (rpm) WSL2 Ubuntu All versions apt (deb)"},{"location":"commands/cuda-install/#other-platforms","title":"Other Platforms","text":"Platform Installation Method Windows 10/11 GUI installer (exe) macOS Not supported (CUDA deprecated) Conda (any platform) <code>conda install cuda-toolkit</code>"},{"location":"commands/cuda-install/#cuda-versions-supported","title":"CUDA Versions Supported","text":"<p>The tool provides installation instructions for:</p> <ul> <li>CUDA 12.6 (Latest stable, requires driver &gt;= 560.xx)</li> <li>CUDA 12.4 (TensorFlow 2.16+ compatible, requires driver &gt;= 550.xx)</li> <li>CUDA 12.1 (PyTorch 2.x sweet spot, requires driver &gt;= 530.xx)</li> <li>CUDA 11.8 (Legacy compatibility, requires driver &gt;= 520.xx)</li> </ul>"},{"location":"commands/cuda-install/#version-recommendation-logic","title":"Version Recommendation Logic","text":"<p>The tool automatically recommends the best CUDA version based on:</p> <ol> <li>Your GPU driver - Maps driver version to max supported CUDA</li> <li>Forward compatibility - Recommends latest stable CUDA your driver supports</li> <li>Library compatibility - Considers PyTorch/TensorFlow requirements</li> </ol> <p>Example Mappings:</p> Driver Version Max CUDA Recommended Toolkit 560.xx 12.6 CUDA 12.6 555.xx 12.5 CUDA 12.4 550.xx 12.4 CUDA 12.4 535.xx 12.2 CUDA 12.1 530.xx 12.1 CUDA 12.1 520.xx 11.8 CUDA 11.8"},{"location":"commands/cuda-install/#common-use-cases","title":"Common Use Cases","text":""},{"location":"commands/cuda-install/#case-1-new-machine-setup","title":"Case 1: New Machine Setup","text":"<pre><code># 1. Check current state\nenv-doctor check\n\n# 2. Get installation instructions\nenv-doctor cuda-install\n\n# 3. Follow the steps shown\n\n# 4. Verify installation\nenv-doctor check\n</code></pre>"},{"location":"commands/cuda-install/#case-2-upgrade-cuda-for-pytorch-compatibility","title":"Case 2: Upgrade CUDA for PyTorch Compatibility","text":"<pre><code># 1. Check current versions\nenv-doctor check\n\n# 2. Get instructions for specific version\nenv-doctor cuda-install 12.4\n\n# 3. Install and verify\nenv-doctor check\n</code></pre>"},{"location":"commands/cuda-install/#case-3-wsl2-gpu-setup","title":"Case 3: WSL2 GPU Setup","text":"<pre><code># Inside WSL2:\n# 1. Check if driver is forwarded\nenv-doctor check\n\n# 2. Get WSL2-specific instructions\nenv-doctor cuda-install\n\n# 3. Follow WSL2 prerequisites carefully (don't install driver in WSL!)\n</code></pre>"},{"location":"commands/cuda-install/#post-installation","title":"Post-Installation","text":"<p>After installing CUDA Toolkit:</p> <ol> <li> <p>Verify installation: <pre><code>nvcc --version\n</code></pre></p> </li> <li> <p>Run full diagnostic: <pre><code>env-doctor check\n</code></pre></p> </li> <li> <p>Install Python libraries: <pre><code>env-doctor install torch\n</code></pre></p> </li> </ol>"},{"location":"commands/cuda-install/#troubleshooting","title":"Troubleshooting","text":""},{"location":"commands/cuda-install/#no-nvidia-driver-detected","title":"\"No NVIDIA driver detected\"","text":"<p>Install the NVIDIA driver first: - Linux: Use your distribution's package manager or NVIDIA's .run installer - Windows: Download from NVIDIA Drivers - WSL2: Install driver on Windows host, not in WSL2</p>"},{"location":"commands/cuda-install/#no-specific-instructions-for-your-platform","title":"\"No specific instructions for your platform\"","text":"<p>The tool will show: - List of available platforms for your CUDA version - Generic download link to NVIDIA's website - Conda installation as universal fallback</p> <p>You can also specify a different CUDA version that might have your platform: <pre><code>env-doctor cuda-install 12.1\n</code></pre></p>"},{"location":"commands/cuda-install/#multiple-cuda-installations","title":"Multiple CUDA Installations","text":"<p>If you already have CUDA installed: - The new installation will be in a versioned directory (e.g., <code>/usr/local/cuda-12.4</code>) - Update <code>CUDA_HOME</code> and <code>PATH</code> to point to the version you want - Use <code>env-doctor cuda-info</code> to see all installations</p>"},{"location":"commands/cuda-install/#integration-with-other-commands","title":"Integration with Other Commands","text":"<p>The <code>cuda-install</code> command works seamlessly with:</p> <ul> <li><code>env-doctor check</code> - See what's currently installed</li> <li><code>env-doctor cuda-info</code> - Detailed analysis of CUDA installations</li> <li><code>env-doctor install &lt;lib&gt;</code> - Get library install commands after CUDA is set up</li> </ul>"},{"location":"commands/cuda-install/#see-also","title":"See Also","text":"<ul> <li><code>check</code> - Diagnose your environment</li> <li><code>cuda-info</code> - Detailed CUDA toolkit analysis</li> <li><code>install</code> - Get safe library install commands</li> <li>WSL2 GPU Guide - Complete WSL2 setup guide</li> </ul>"},{"location":"commands/cudnn-info/","title":"cudnn-info","text":"<p>Detect and validate cuDNN library installations.</p>"},{"location":"commands/cudnn-info/#usage","title":"Usage","text":"<pre><code>env-doctor cudnn-info\n</code></pre>"},{"location":"commands/cudnn-info/#what-it-shows","title":"What It Shows","text":""},{"location":"commands/cudnn-info/#cudnn-detection","title":"cuDNN Detection","text":"<ul> <li>cuDNN version and build information</li> <li>Library file locations</li> <li>Header file locations</li> </ul>"},{"location":"commands/cudnn-info/#installation-validation","title":"Installation Validation","text":"<ul> <li>Multiple installation detection</li> <li>Symlink validation (Linux)</li> <li>PATH configuration (Windows)</li> </ul>"},{"location":"commands/cudnn-info/#compatibility","title":"Compatibility","text":"<ul> <li>CUDA version compatibility</li> <li>Version requirements for popular frameworks</li> </ul>"},{"location":"commands/cudnn-info/#example-output","title":"Example Output","text":"<pre><code>\ud83e\udde0 cuDNN ANALYSIS\n============================================================\n\n\ud83d\udcda cuDNN Installation:\n\n  Version: 8.9.7\n  CUDA Version: 12.x\n\n  Libraries:\n    \u2705 libcudnn.so.8.9.7\n    \u2514\u2500 Path: /usr/local/cuda-12.1/lib64/libcudnn.so.8\n\n  Headers:\n    \u2705 cudnn.h\n    \u2514\u2500 Path: /usr/local/cuda-12.1/include/cudnn.h\n\n\ud83d\udd17 Symlinks (Linux):\n\n  libcudnn.so \u2192 libcudnn.so.8 \u2192 libcudnn.so.8.9.7\n  \u2705 Symlink chain is valid\n\n\ud83c\udfae Compatibility:\n\n  \u2705 cuDNN 8.9.7 is compatible with CUDA 12.1\n  \u2705 Meets PyTorch 2.1+ requirements (cuDNN 8.5+)\n  \u2705 Meets TensorFlow 2.15+ requirements (cuDNN 8.6+)\n</code></pre>"},{"location":"commands/cudnn-info/#multiple-installations","title":"Multiple Installations","text":"<pre><code>\u26a0\ufe0f  Multiple cuDNN installations detected\n\n  /usr/local/cuda-12.1/lib64/libcudnn.so.8.9.7\n  /usr/lib/x86_64-linux-gnu/libcudnn.so.8.6.0\n\n\ud83d\udca1 Recommendations:\n  - Ensure LD_LIBRARY_PATH prioritizes the correct version\n  - Consider removing older installations to avoid conflicts\n</code></pre>"},{"location":"commands/cudnn-info/#common-issues","title":"Common Issues","text":""},{"location":"commands/cudnn-info/#missing-cudnn","title":"Missing cuDNN","text":"<pre><code>\u274c cuDNN not found\n\n\ud83d\udca1 Installation:\n  1. Download from https://developer.nvidia.com/cudnn\n  2. Extract to /usr/local/cuda-12.1/\n  3. Or install via package manager:\n     sudo apt install libcudnn8 libcudnn8-dev\n</code></pre>"},{"location":"commands/cudnn-info/#broken-symlinks","title":"Broken Symlinks","text":"<pre><code>\u274c Broken symlink detected\n\n  libcudnn.so \u2192 libcudnn.so.8 (missing)\n\n\ud83d\udca1 Fix:\n  cd /usr/local/cuda/lib64\n  sudo ln -sf libcudnn.so.8.9.7 libcudnn.so.8\n  sudo ln -sf libcudnn.so.8 libcudnn.so\n</code></pre>"},{"location":"commands/cudnn-info/#version-mismatch","title":"Version Mismatch","text":"<pre><code>\u26a0\ufe0f  cuDNN version may not be optimal\n\n  Installed: cuDNN 8.2.0\n  PyTorch 2.1 recommends: cuDNN 8.5+\n\n\ud83d\udca1 Consider upgrading cuDNN for better performance\n</code></pre>"},{"location":"commands/cudnn-info/#advanced-options","title":"Advanced Options","text":""},{"location":"commands/cudnn-info/#json-output","title":"JSON Output","text":"<pre><code>env-doctor cudnn-info --json\n</code></pre>"},{"location":"commands/cudnn-info/#see-also","title":"See Also","text":"<ul> <li>cuda-info - CUDA toolkit analysis</li> <li>check - Full environment diagnosis</li> </ul>"},{"location":"commands/debug/","title":"debug","text":"<p>Get detailed information from all detectors for troubleshooting.</p>"},{"location":"commands/debug/#usage","title":"Usage","text":"<pre><code>env-doctor debug\n</code></pre>"},{"location":"commands/debug/#what-it-shows","title":"What It Shows","text":""},{"location":"commands/debug/#all-detector-results","title":"All Detector Results","text":"<ul> <li>Raw output from every registered detector</li> <li>Detection status (success, warning, error, not_found)</li> <li>Version information</li> </ul>"},{"location":"commands/debug/#detection-metadata","title":"Detection Metadata","text":"<ul> <li>Internal detection methods used</li> <li>File paths examined</li> <li>Detailed status information</li> </ul>"},{"location":"commands/debug/#registry-information","title":"Registry Information","text":"<ul> <li>List of all available detectors</li> <li>Detector execution order</li> </ul>"},{"location":"commands/debug/#error-details","title":"Error Details","text":"<ul> <li>Full exception traces</li> <li>Diagnostic information for failures</li> </ul>"},{"location":"commands/debug/#example-output","title":"Example Output","text":"<pre><code>\ud83d\udd0d DEBUG MODE - Detailed Detector Information\n============================================================\nRegistered Detectors: cuda_toolkit, nvidia_driver, python_library, wsl2, cudnn\n\n--- WSL2 ---\nStatus: Status.SUCCESS\nComponent: wsl2\nVersion: wsl2\nMetadata: {\n  'environment': 'WSL2',\n  'gpu_forwarding': 'enabled',\n  'wsl_lib_path': '/usr/lib/wsl/lib',\n  'libcuda_found': True\n}\n\n--- NVIDIA DRIVER ---\nStatus: Status.SUCCESS\nComponent: nvidia_driver\nVersion: 535.146.02\nPath: /usr/lib/x86_64-linux-gnu/libnvidia-ml.so.1\nMetadata: {\n  'max_cuda_version': '12.2',\n  'detection_method': 'nvml',\n  'gpu_name': 'NVIDIA GeForce RTX 3090',\n  'gpu_memory': 24576\n}\n\n--- CUDA TOOLKIT ---\nStatus: Status.SUCCESS\nComponent: cuda_toolkit\nVersion: 12.1.1\nPath: /usr/local/cuda-12.1\nMetadata: {\n  'nvcc_path': '/usr/local/cuda-12.1/bin/nvcc',\n  'cuda_home': '/usr/local/cuda-12.1',\n  'multiple_installations': ['/usr/local/cuda-12.1', '/usr/local/cuda-11.8']\n}\n\n--- CUDNN ---\nStatus: Status.SUCCESS\nComponent: cudnn\nVersion: 8.9.7\nPath: /usr/local/cuda-12.1/lib64/libcudnn.so.8\nMetadata: {\n  'cuda_version': '12.x',\n  'header_path': '/usr/local/cuda-12.1/include/cudnn.h'\n}\n\n--- PYTHON LIBRARIES ---\nStatus: Status.SUCCESS\nComponent: python_library\nLibraries: {\n  'torch': {\n    'version': '2.1.0+cu121',\n    'cuda_version': '12.1',\n    'detection_method': 'import'\n  },\n  'tensorflow': {\n    'status': 'not_installed'\n  }\n}\n</code></pre>"},{"location":"commands/debug/#when-to-use-debug-mode","title":"When to Use Debug Mode","text":""},{"location":"commands/debug/#unexpected-results","title":"Unexpected Results","text":"<p>When <code>env-doctor check</code> shows results that don't match your expectations:</p> <pre><code># Regular check shows CUDA not found, but you know it's installed\nenv-doctor check\n# Shows: \u26a0\ufe0f No system CUDA toolkit found\n\n# Debug to see what happened\nenv-doctor debug\n# Shows: Detection attempted at /usr/local/cuda (symlink broken)\n</code></pre>"},{"location":"commands/debug/#contributing-or-reporting-issues","title":"Contributing or Reporting Issues","text":"<p>Include debug output when:</p> <ul> <li>Reporting bugs</li> <li>Contributing new detectors</li> <li>Validating behavior changes</li> </ul>"},{"location":"commands/debug/#understanding-detection-methods","title":"Understanding Detection Methods","text":"<p>Debug shows how each component was detected:</p> <ul> <li>NVML for driver detection</li> <li>File system search for CUDA</li> <li>Python imports for libraries</li> </ul>"},{"location":"commands/debug/#see-also","title":"See Also","text":"<ul> <li>check - Standard environment check</li> <li>Architecture - How detectors work</li> </ul>"},{"location":"commands/docker-compose/","title":"docker-compose","text":"<p>Validate docker-compose.yml files for proper GPU device configuration.</p>"},{"location":"commands/docker-compose/#usage","title":"Usage","text":"<pre><code>env-doctor docker-compose [PATH]\n</code></pre> <p>If no path is provided, looks for <code>docker-compose.yml</code> or <code>docker-compose.yaml</code> in the current directory.</p>"},{"location":"commands/docker-compose/#what-it-validates","title":"What It Validates","text":""},{"location":"commands/docker-compose/#gpu-device-configuration","title":"GPU Device Configuration","text":"<ul> <li>Checks for <code>deploy.resources.reservations.devices</code></li> <li>Validates <code>driver: nvidia</code> is specified</li> <li>Ensures <code>capabilities: [gpu]</code> is set</li> </ul>"},{"location":"commands/docker-compose/#deprecated-syntax","title":"Deprecated Syntax","text":"<ul> <li>Flags old <code>runtime: nvidia</code> approach</li> <li>Provides migration path to new syntax</li> </ul>"},{"location":"commands/docker-compose/#multi-service-configuration","title":"Multi-Service Configuration","text":"<ul> <li>Warns about GPU resource sharing between services</li> <li>Validates each service with GPU requirements</li> </ul>"},{"location":"commands/docker-compose/#host-requirements","title":"Host Requirements","text":"<ul> <li>Checks for nvidia-container-toolkit</li> <li>Validates Docker GPU support</li> </ul>"},{"location":"commands/docker-compose/#example-output","title":"Example Output","text":"<pre><code>\ud83d\udc33  DOCKER COMPOSE VALIDATION: docker-compose.yml\n\n\u274c  ERRORS (1):\n------------------------------------------------------------\n\nService 'ml-training':\n  Issue: Missing GPU device configuration\n  Fix:   Add GPU device configuration under deploy.resources.reservations.devices\n\n  Suggested fix:\n    deploy:\n      resources:\n        reservations:\n          devices:\n            - driver: nvidia\n              count: all\n              capabilities: [gpu]\n\n\u26a0\ufe0f   WARNINGS (1):\n------------------------------------------------------------\n\nService 'legacy-app':\n  Issue: Deprecated 'runtime: nvidia' syntax\n  Fix:   Use the new 'deploy.resources.reservations.devices' syntax instead\n\nSUMMARY:\n  \u274c Errors:   1\n  \u26a0\ufe0f  Warnings: 1\n  \u2139\ufe0f  Info:     0\n</code></pre>"},{"location":"commands/docker-compose/#correct-gpu-configuration","title":"Correct GPU Configuration","text":""},{"location":"commands/docker-compose/#modern-syntax-compose-v24","title":"Modern Syntax (Compose v2.4+)","text":"<pre><code>services:\n  ml-training:\n    image: pytorch/pytorch:2.1.0-cuda12.1-cudnn8-runtime\n    deploy:\n      resources:\n        reservations:\n          devices:\n            - driver: nvidia\n              count: all  # or specific number: count: 1\n              capabilities: [gpu]\n</code></pre>"},{"location":"commands/docker-compose/#device-selection","title":"Device Selection","text":"<pre><code># Use all GPUs\ncount: all\n\n# Use specific number of GPUs\ncount: 2\n\n# Use specific GPU by ID\ndevice_ids: ['0', '2']\n</code></pre>"},{"location":"commands/docker-compose/#deprecated-syntax_1","title":"Deprecated Syntax","text":"<pre><code># \u274c Old syntax (still works but deprecated)\nservices:\n  app:\n    runtime: nvidia\n\n# \u2705 New syntax\nservices:\n  app:\n    deploy:\n      resources:\n        reservations:\n          devices:\n            - driver: nvidia\n              capabilities: [gpu]\n</code></pre>"},{"location":"commands/docker-compose/#common-issues","title":"Common Issues","text":""},{"location":"commands/docker-compose/#missing-nvidia-container-toolkit","title":"Missing nvidia-container-toolkit","text":"<pre><code>\u274c nvidia-container-toolkit not detected on host\n\n\ud83d\udca1 Installation:\n  # Ubuntu/Debian\n  distribution=$(. /etc/os-release;echo $ID$VERSION_ID)\n  curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -\n  curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | \\\n    sudo tee /etc/apt/sources.list.d/nvidia-docker.list\n  sudo apt update &amp;&amp; sudo apt install nvidia-container-toolkit\n  sudo systemctl restart docker\n</code></pre>"},{"location":"commands/docker-compose/#gpu-sharing-warning","title":"GPU Sharing Warning","text":"<pre><code>\u26a0\ufe0f  Multiple services reserving GPU resources\n\n  Services: ml-training, inference-api\n\n\ud83d\udca1 Note:\n  GPU memory is shared between containers.\n  Ensure total VRAM requirements don't exceed available memory.\n</code></pre>"},{"location":"commands/docker-compose/#see-also","title":"See Also","text":"<ul> <li>dockerfile - Validate Dockerfile</li> </ul>"},{"location":"commands/dockerfile/","title":"dockerfile","text":"<p>Validate Dockerfiles for GPU/CUDA configuration issues before building.</p>"},{"location":"commands/dockerfile/#usage","title":"Usage","text":"<pre><code>env-doctor dockerfile [PATH]\n</code></pre> <p>If no path is provided, looks for <code>Dockerfile</code> in the current directory.</p>"},{"location":"commands/dockerfile/#what-it-validates","title":"What It Validates","text":""},{"location":"commands/dockerfile/#base-images","title":"Base Images","text":"<ul> <li>Detects CPU-only images (<code>python:3.10</code>, <code>ubuntu:22.04</code>)</li> <li>Provides DB-driven GPU base image recommendations</li> <li>Suggests appropriate runtime vs devel images</li> </ul>"},{"location":"commands/dockerfile/#pytorch-installation","title":"PyTorch Installation","text":"<ul> <li>Ensures <code>pip install torch</code> has correct <code>--index-url</code></li> <li>Validates version compatibility with base image CUDA</li> <li>Uses verified install commands from the database</li> </ul>"},{"location":"commands/dockerfile/#library-compatibility","title":"Library Compatibility","text":"<ul> <li>Validates pinned versions against DB-verified combinations</li> <li>Checks multi-library compatibility (torch + tensorflow + jax)</li> <li>Flags deprecated packages (<code>tensorflow-gpu</code>)</li> </ul>"},{"location":"commands/dockerfile/#build-requirements","title":"Build Requirements","text":"<ul> <li>Detects compilation requirements (flash-attn, xformers)</li> <li>Enforces <code>-devel</code> base images when needed</li> <li>Warns about unnecessary toolkit installs</li> </ul>"},{"location":"commands/dockerfile/#common-mistakes","title":"Common Mistakes","text":"<ul> <li>Flags NVIDIA driver installs (must be on host)</li> <li>Warns about bloating images with unnecessary packages</li> </ul>"},{"location":"commands/dockerfile/#example-output","title":"Example Output","text":"<pre><code>\ud83d\udc33  DOCKERFILE VALIDATION: Dockerfile\n\n\u274c  ERRORS (2):\n------------------------------------------------------------\n\nLine 1:\n  Issue: CPU-only base image detected: python:3.10\n  Fix:   Use a GPU-enabled base image\n\n  Suggested fix:\n    FROM nvidia/cuda:12.1.0-runtime-ubuntu22.04\n    # Or: FROM pytorch/pytorch:2.1.0-cuda12.1-cudnn8-runtime\n    # Or: FROM tensorflow/tensorflow:latest-gpu\n\nLine 8:\n  Issue: PyTorch installation missing --index-url flag\n  Fix:   Add --index-url to install the correct CUDA version\n\n  Suggested fix:\n    RUN pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121\n\n\u26a0\ufe0f   WARNINGS (1):\n------------------------------------------------------------\n\nLine 15:\n  Issue: Installing CUDA toolkit in container\n  Fix:   Use a CUDA base image instead to reduce image size\n\nSUMMARY:\n  \u274c Errors:   2\n  \u26a0\ufe0f  Warnings: 1\n  \u2139\ufe0f  Info:     0\n</code></pre>"},{"location":"commands/dockerfile/#validation-rules","title":"Validation Rules","text":""},{"location":"commands/dockerfile/#base-image-rules","title":"Base Image Rules","text":"Image Type Recommendation <code>python:*</code> Use <code>nvidia/cuda:*</code> or <code>pytorch/pytorch:*</code> <code>ubuntu:*</code> Use <code>nvidia/cuda:*-base-ubuntu*</code> <code>nvidia/cuda:*-runtime-*</code> Good for inference <code>nvidia/cuda:*-devel-*</code> Required for compilation"},{"location":"commands/dockerfile/#pytorch-install-rules","title":"PyTorch Install Rules","text":"<pre><code># \u274c Wrong - gets whatever CUDA version pip decides\nRUN pip install torch\n\n# \u2705 Correct - explicit CUDA version\nRUN pip install torch --index-url https://download.pytorch.org/whl/cu121\n</code></pre>"},{"location":"commands/dockerfile/#compilation-detection","title":"Compilation Detection","text":"<p>When these packages are detected, <code>-devel</code> base image is required:</p> <ul> <li><code>flash-attn</code></li> <li><code>xformers</code> (when building from source)</li> <li>Any package with <code>--no-binary</code></li> </ul>"},{"location":"commands/dockerfile/#see-also","title":"See Also","text":"<ul> <li>docker-compose - Validate docker-compose.yml</li> </ul>"},{"location":"commands/install/","title":"install","text":"<p>Get the safe install command for GPU libraries that matches your driver.</p>"},{"location":"commands/install/#usage","title":"Usage","text":"<pre><code>env-doctor install &lt;library&gt;\n</code></pre>"},{"location":"commands/install/#supported-libraries","title":"Supported Libraries","text":"<ul> <li><code>torch</code> / <code>pytorch</code> - PyTorch with correct CUDA wheels</li> <li><code>tensorflow</code> - TensorFlow with GPU support</li> <li><code>jax</code> - JAX with CUDA support</li> </ul>"},{"location":"commands/install/#how-it-works","title":"How It Works","text":"<ol> <li>Detects your NVIDIA driver version</li> <li>Determines the maximum CUDA version your driver supports</li> <li>Looks up the correct wheel URL for that CUDA version</li> <li>Outputs the exact <code>pip install</code> command</li> </ol>"},{"location":"commands/install/#example","title":"Example","text":"<pre><code>env-doctor install torch\n</code></pre> <p>Output:</p> <pre><code>\ud83d\udd0d Detecting your GPU environment...\n\n\ud83c\udfae GPU Driver: 535.146.02\n   \u2514\u2500 Max CUDA: 12.2\n\n\u2b07\ufe0f Run this command to install the SAFE version:\n---------------------------------------------------\npip install torch torchvision --index-url https://download.pytorch.org/whl/cu121\n---------------------------------------------------\n\n\ud83d\udca1 This installs PyTorch built for CUDA 12.1, which is compatible\n   with your driver (supports up to CUDA 12.2).\n</code></pre>"},{"location":"commands/install/#why-this-matters","title":"Why This Matters","text":"<p>The default <code>pip install torch</code> gives you the latest wheel, which might be built for CUDA 12.4. If your driver only supports CUDA 11.8, you'll get cryptic errors like:</p> <pre><code>RuntimeError: CUDA error: no kernel image is available for execution on the device\n</code></pre> <p>Env-Doctor prevents this by prescribing the correct version for your hardware.</p>"},{"location":"commands/install/#common-scenarios","title":"Common Scenarios","text":""},{"location":"commands/install/#older-driver","title":"Older Driver","text":"<pre><code>$ env-doctor install torch\n\n\ud83c\udfae GPU Driver: 470.82.01\n   \u2514\u2500 Max CUDA: 11.4\n\n\u2b07\ufe0f Run this command to install the SAFE version:\n---------------------------------------------------\npip install torch torchvision --index-url https://download.pytorch.org/whl/cu113\n---------------------------------------------------\n</code></pre>"},{"location":"commands/install/#no-gpu-detected","title":"No GPU Detected","text":"<pre><code>$ env-doctor install torch\n\n\u26a0\ufe0f  No NVIDIA GPU detected\n\n\u2b07\ufe0f Installing CPU-only version:\n---------------------------------------------------\npip install torch torchvision --index-url https://download.pytorch.org/whl/cpu\n---------------------------------------------------\n</code></pre>"},{"location":"commands/install/#see-also","title":"See Also","text":"<ul> <li>check - Full environment diagnosis</li> <li>model - Check model VRAM requirements</li> </ul>"},{"location":"commands/model/","title":"model","text":"<p>Check if an AI model fits on your GPU before downloading.</p>"},{"location":"commands/model/#usage","title":"Usage","text":"<pre><code>env-doctor model &lt;model-name&gt;\n</code></pre>"},{"location":"commands/model/#options","title":"Options","text":"Option Description <code>--list</code> List all available models in local database <code>--precision &lt;type&gt;</code> Check specific precision (fp32, fp16, bf16, int8, int4, fp8)"},{"location":"commands/model/#example","title":"Example","text":"<pre><code>env-doctor model llama-3-8b\n</code></pre> <p>Output:</p> <pre><code>\ud83e\udd16  Checking: LLAMA-3-8B\n    Parameters: 8.0B\n    HuggingFace: meta-llama/Meta-Llama-3-8B\n\n\ud83d\udda5\ufe0f   Your Hardware:\n    RTX 3090 (24GB VRAM)\n\n\ud83d\udcbe  VRAM Requirements &amp; Compatibility\n\n  \u2705  FP16: 19.2GB (measured) - 4.8GB free\n  \u2705  INT4:  4.8GB (estimated) - 19.2GB free\n\n\u2705  This model WILL FIT on your GPU!\n\n\ud83d\udca1  Recommendations:\n1. Use fp16 for best quality on your GPU\n</code></pre>"},{"location":"commands/model/#listing-available-models","title":"Listing Available Models","text":"<pre><code>env-doctor model --list\n</code></pre> <p>Output:</p> <pre><code>\ud83d\udccb Available Models\n\nLLMs:\n  llama-3-8b, llama-3-70b, llama-3-405b\n  mistral-7b, mixtral-8x7b\n  qwen-7b, qwen-14b, qwen-72b\n\nDiffusion:\n  stable-diffusion-1.5, stable-diffusion-xl, stable-diffusion-3\n  flux-schnell, flux-dev\n\nAudio:\n  whisper-tiny, whisper-base, whisper-small\n  whisper-medium, whisper-large, whisper-large-v3\n\nLanguage:\n  bert-base, bert-large\n  t5-small, t5-base, t5-large\n</code></pre>"},{"location":"commands/model/#checking-specific-precision","title":"Checking Specific Precision","text":"<pre><code>env-doctor model stable-diffusion-xl --precision int4\n</code></pre> <p>Output:</p> <pre><code>\ud83e\udd16  Checking: STABLE-DIFFUSION-XL\n    Parameters: 6.6B\n\n\ud83d\udda5\ufe0f   Your Hardware:\n    RTX 3060 (12GB VRAM)\n\n\ud83d\udcbe  INT4 Requirements:\n    ~2.5GB VRAM (estimated)\n\n\u2705  This model WILL FIT at INT4 precision!\n</code></pre>"},{"location":"commands/model/#model-database","title":"Model Database","text":"<p>env-doctor includes a curated local database of 75+ popular models with measured VRAM usage, plus access to thousands of models via HuggingFace Hub API.</p>"},{"location":"commands/model/#supported-model-categories","title":"Supported Model Categories","text":"<p>The local database includes models across multiple categories:</p> <ul> <li>LLMs: Llama 3 (8B-405B), Mistral, Mixtral, Qwen, Gemma, Phi, CodeLlama</li> <li>Diffusion: Stable Diffusion (1.5, XL, 3), Flux (Schnell, Dev), Pixart</li> <li>Audio: Whisper (all sizes), Bark</li> <li>Vision: CLIP, SAM, DINOv2</li> <li>Language: BERT, T5, RoBERTa, DistilBERT</li> </ul>"},{"location":"commands/model/#example-models-from-local-database","title":"Example Models from Local Database","text":"Category Example Models Typical VRAM (FP16) Small LLMs Llama-3-8B, Mistral-7B, Gemma-7B 14-19GB Large LLMs Llama-3-70B, Mixtral-8x7B 93-140GB Diffusion SD 1.5, SD XL, Flux Schnell 4-12GB Audio Whisper Tiny/Base/Large 1-10GB Vision CLIP, SAM, BERT 0.5-2GB <p>Beyond the Local Database</p> <p>Can't find your model? No problem! Any public HuggingFace model can be checked automatically. See HuggingFace API Integration below.</p>"},{"location":"commands/model/#model-aliases","title":"Model Aliases","text":"<p>Common aliases are supported for quick access:</p> <ul> <li><code>sdxl</code> \u2192 <code>stable-diffusion-xl</code></li> <li><code>sd15</code> \u2192 <code>stable-diffusion-1.5</code></li> <li><code>llama3</code> \u2192 <code>llama-3-8b</code></li> <li><code>gemma</code> \u2192 <code>gemma-7b</code></li> <li><code>phi2</code> \u2192 <code>phi-2</code></li> <li><code>codellama</code> \u2192 <code>codellama-7b</code></li> <li><code>clip</code> \u2192 <code>clip-vit-base</code></li> <li><code>sam</code> \u2192 <code>sam-vit-base</code></li> </ul>"},{"location":"commands/model/#huggingface-api-integration","title":"HuggingFace API Integration","text":"<p>New Feature</p> <p>Models not in the local database are automatically fetched from HuggingFace Hub!</p>"},{"location":"commands/model/#3-tier-fallback-system","title":"3-Tier Fallback System","text":"<p>When you query a model, env-doctor uses a smart 3-tier lookup:</p> <pre><code>Tier 1: Local Database (75+ models) \u2192 Fastest, measured VRAM values\n    \u2193 (if not found)\nTier 2: HF Cache \u2192 Previously fetched models, no network call\n    \u2193 (if not found)\nTier 3: HuggingFace Hub API \u2192 Dynamic fetch, then cached\n</code></pre>"},{"location":"commands/model/#checking-any-huggingface-model","title":"Checking Any HuggingFace Model","text":"<p>You can check any public model from HuggingFace Hub:</p> <pre><code># Using HuggingFace model ID\nenv-doctor model bert-base-uncased\nenv-doctor model sentence-transformers/all-MiniLM-L6-v2\nenv-doctor model distilbert-base-uncased\n</code></pre> <p>Output for HuggingFace-fetched model:</p> <pre><code>\ud83e\udd16  Checking: BERT-BASE-UNCASED\n    (Fetched from HuggingFace API - cached for future use)\n    Parameters: 0.11B\n    HuggingFace: bert-base-uncased\n\n\ud83d\udda5\ufe0f   Your Hardware:\n    RTX 3090 (24GB VRAM)\n\n\ud83d\udcbe  VRAM Requirements &amp; Compatibility\n  \u2705  FP16:  264 MB - Fits easily!\n\n\ud83d\udca1  Recommendations:\n1. Use fp16 for best quality on your GPU\n</code></pre>"},{"location":"commands/model/#automatic-caching","title":"Automatic Caching","text":"<p>Once fetched, models are cached in the local database for instant lookup on future queries - no network calls needed!</p> <pre><code># First call: fetches from HuggingFace (2-3 seconds)\nenv-doctor model sentence-transformers/all-MiniLM-L6-v2\n\n# Second call: uses cache (instant)\nenv-doctor model sentence-transformers/all-MiniLM-L6-v2\n</code></pre>"},{"location":"commands/model/#limitations","title":"Limitations","text":"<p>Gated Models</p> <p>HuggingFace models that require authentication (signup/access request) cannot be fetched automatically. Use models from the local database or public HuggingFace models.</p>"},{"location":"commands/model/#when-a-model-wont-fit","title":"When a Model Won't Fit","text":"<pre><code>env-doctor model llama-3-70b\n</code></pre> <pre><code>\ud83e\udd16  Checking: LLAMA-3-70B\n    Parameters: 70B\n\n\ud83d\udda5\ufe0f   Your Hardware:\n    RTX 3090 (24GB VRAM)\n\n\ud83d\udcbe  VRAM Requirements:\n    FP16: ~140GB\n    INT4: ~35GB\n\n\u274c  This model will NOT fit on your GPU\n\n\ud83d\udca1  Recommendations:\n1. Try llama-3-8b (same family, fits in 24GB)\n2. Use INT4 quantization with 2x RTX 3090\n3. Consider cloud GPU (A100 80GB)\n</code></pre>"},{"location":"commands/model/#how-vram-is-calculated","title":"How VRAM Is Calculated","text":"<p>For models with measured data, we use real-world VRAM usage from testing.</p> <p>For other models, we estimate using:</p> Precision Formula FP32 params \u00d7 4 bytes FP16 params \u00d7 2 bytes INT8 params \u00d7 1 byte + overhead INT4 params \u00d7 0.5 bytes + overhead <p>Overhead</p> <p>Actual VRAM usage includes KV cache, activations, and framework overhead. Our estimates include a ~20% buffer for this.</p>"},{"location":"commands/model/#see-also","title":"See Also","text":"<ul> <li>check - Environment diagnosis</li> <li>install - Get safe install commands</li> </ul>"},{"location":"commands/python-compat/","title":"python-compat","text":"<p>Check Python version compatibility with installed AI libraries and detect dependency cascades.</p>"},{"location":"commands/python-compat/#usage","title":"Usage","text":"<pre><code>env-doctor python-compat [--json]\n</code></pre>"},{"location":"commands/python-compat/#description","title":"Description","text":"<p>The <code>python-compat</code> command checks if your current Python version is compatible with installed AI libraries like PyTorch, TensorFlow, JAX, NumPy, SciPy, and others. It identifies:</p> <ul> <li>Version conflicts: When your Python version is outside a library's supported range</li> <li>Dependency cascades: When one library's constraint forces version limits on downstream packages</li> </ul>"},{"location":"commands/python-compat/#options","title":"Options","text":"Option Description <code>--json</code> Output results in JSON format for programmatic consumption"},{"location":"commands/python-compat/#output","title":"Output","text":""},{"location":"commands/python-compat/#human-readable-format-default","title":"Human-readable format (default)","text":"<pre><code>\ud83d\udc0d  PYTHON VERSION COMPATIBILITY CHECK\n============================================================\nPython Version: 3.13 (3.13.0)\nLibraries Checked: 2\n\n\u274c  2 compatibility issue(s) found:\n\n    tensorflow:\n      tensorflow supports Python &lt;=3.12, but you have Python 3.13\n      Note: TensorFlow 2.15+ requires Python 3.9-3.12. Python 3.13 not yet supported.\n\n    torch:\n      torch supports Python &lt;=3.12, but you have Python 3.13\n      Note: PyTorch 2.x supports Python 3.9-3.12. Python 3.13 support experimental.\n\n\u26a0\ufe0f   Dependency Cascades:\n    tensorflow [high]: TensorFlow's Python ceiling propagates to keras and tensorboard\n      Affected: keras, tensorboard, tensorflow-estimator\n    torch [high]: PyTorch's Python version constraint affects all torch ecosystem packages\n      Affected: torchvision, torchaudio, triton\n\n\ud83d\udca1  Consider using Python 3.12 or lower for full compatibility\n\n\ud83d\udca1  Cascade: tensorflow constraint also affects: keras, tensorboard, tensorflow-estimator\n\n\ud83d\udca1  Cascade: torch constraint also affects: torchvision, torchaudio, triton\n\n============================================================\n</code></pre>"},{"location":"commands/python-compat/#json-format-json","title":"JSON format (<code>--json</code>)","text":"<pre><code>{\n  \"component\": \"python_compat\",\n  \"status\": \"error\",\n  \"detected\": false,\n  \"version\": \"3.13\",\n  \"path\": null,\n  \"metadata\": {\n    \"python_full_version\": \"3.13.0\",\n    \"conflicts\": [\n      {\n        \"library\": \"tensorflow\",\n        \"min_version\": \"3.9\",\n        \"max_version\": \"3.12\",\n        \"notes\": \"TensorFlow 2.15+ requires Python 3.9-3.12. Python 3.13 not yet supported.\",\n        \"type\": \"above_maximum\",\n        \"message\": \"tensorflow supports Python &lt;=3.12, but you have Python 3.13\"\n      },\n      {\n        \"library\": \"torch\",\n        \"min_version\": \"3.9\",\n        \"max_version\": \"3.12\",\n        \"notes\": \"PyTorch 2.x supports Python 3.9-3.12. Python 3.13 support experimental.\",\n        \"type\": \"above_maximum\",\n        \"message\": \"torch supports Python &lt;=3.12, but you have Python 3.13\"\n      }\n    ],\n    \"cascades\": [\n      {\n        \"root_library\": \"tensorflow\",\n        \"affected_dependencies\": [\"keras\", \"tensorboard\", \"tensorflow-estimator\"],\n        \"severity\": \"high\",\n        \"description\": \"TensorFlow's Python ceiling propagates to keras and tensorboard\"\n      },\n      {\n        \"root_library\": \"torch\",\n        \"affected_dependencies\": [\"torchvision\", \"torchaudio\", \"triton\"],\n        \"severity\": \"high\",\n        \"description\": \"PyTorch's Python version constraint affects all torch ecosystem packages\"\n      }\n    ],\n    \"constraints_checked\": 2\n  },\n  \"issues\": [\n    \"tensorflow supports Python &lt;=3.12, but you have Python 3.13\",\n    \"torch supports Python &lt;=3.12, but you have Python 3.13\"\n  ],\n  \"recommendations\": [\n    \"Consider using Python 3.12 or lower for full compatibility\",\n    \"Cascade: tensorflow constraint also affects: keras, tensorboard, tensorflow-estimator\",\n    \"Cascade: torch constraint also affects: torchvision, torchaudio, triton\"\n  ]\n}\n</code></pre>"},{"location":"commands/python-compat/#exit-codes","title":"Exit Codes","text":"Code Meaning <code>0</code> All installed libraries are compatible with current Python version <code>1</code> Compatibility issues detected (conflicts or warnings)"},{"location":"commands/python-compat/#use-cases","title":"Use Cases","text":""},{"location":"commands/python-compat/#1-before-upgrading-python","title":"1. Before upgrading Python","text":"<p>Check if your installed libraries support the new Python version:</p> <pre><code># On Python 3.12\nenv-doctor python-compat\n\n# Check what happens if you upgrade\npyenv install 3.13\npyenv shell 3.13\nenv-doctor python-compat\n</code></pre>"},{"location":"commands/python-compat/#2-after-fresh-python-install","title":"2. After fresh Python install","text":"<p>Verify that your planned AI stack will work:</p> <pre><code>python3.13 -m venv venv\nsource venv/bin/activate\npip install torch tensorflow\nenv-doctor python-compat\n</code></pre>"},{"location":"commands/python-compat/#3-cicd-pipelines","title":"3. CI/CD pipelines","text":"<p>Ensure Python version compatibility in automated workflows:</p> <pre><code>- name: Check Python compatibility\n  run: |\n    pip install env-doctor\n    env-doctor python-compat --json\n</code></pre>"},{"location":"commands/python-compat/#4-debugging-import-errors","title":"4. Debugging import errors","text":"<p>When libraries fail to import, check if Python version is the issue:</p> <pre><code># If you see: \"ImportError: DLL load failed\" or \"ModuleNotFoundError\"\nenv-doctor python-compat\n</code></pre>"},{"location":"commands/python-compat/#what-libraries-are-checked","title":"What Libraries Are Checked?","text":"<p>The detector currently tracks compatibility for:</p> <ul> <li>Core AI frameworks: PyTorch, TensorFlow, JAX</li> <li>Scientific computing: NumPy, SciPy</li> <li>ML tools: ONNX Runtime, Transformers</li> <li>GPU acceleration: Triton</li> </ul> <p>The compatibility data is maintained in <code>python_compatibility.json</code> and updated regularly.</p>"},{"location":"commands/python-compat/#understanding-dependency-cascades","title":"Understanding Dependency Cascades","text":"<p>A dependency cascade occurs when a root library's Python version constraint propagates to all its dependencies. For example:</p> <pre><code>tensorflow (requires Python \u22643.12)\n  \u251c\u2500 keras (forced to \u22643.12)\n  \u251c\u2500 tensorboard (forced to \u22643.12)\n  \u2514\u2500 tensorflow-estimator (forced to \u22643.12)\n</code></pre> <p>Even if <code>keras</code> itself supports Python 3.13, it can't be used with TensorFlow on Python 3.13 because TensorFlow is the root constraint.</p> <p>Cascades are marked with severity: - high: Affects many popular packages - medium: Affects a moderate number of packages - low: Limited impact</p>"},{"location":"commands/python-compat/#integration-with-check-command","title":"Integration with <code>check</code> command","text":"<p>The <code>python-compat</code> detector is automatically included in the main <code>check</code> command:</p> <pre><code>env-doctor check\n</code></pre> <p>Output will include a Python compatibility section: <pre><code>\u2705  Python 3.12: Compatible with all 3 checked libraries\n</code></pre></p>"},{"location":"commands/python-compat/#related-commands","title":"Related Commands","text":"<ul> <li><code>check</code> - Full environment diagnosis (includes Python compatibility)</li> <li><code>install</code> - Get safe install commands for libraries</li> <li><code>debug</code> - Detailed detector output including Python compatibility</li> </ul>"},{"location":"commands/python-compat/#troubleshooting","title":"Troubleshooting","text":""},{"location":"commands/python-compat/#no-libraries-checked-constraints_checked-0","title":"No libraries checked (constraints_checked: 0)","text":"<p>This means none of the tracked libraries are installed. Install at least one AI library:</p> <pre><code>pip install torch\nenv-doctor python-compat\n</code></pre>"},{"location":"commands/python-compat/#false-positives","title":"False positives","text":"<p>The compatibility data is based on library documentation and may lag behind actual support. If you believe a conflict is incorrect, please open an issue with: - Python version - Library version - Evidence of compatibility (e.g., successful import)</p>"},{"location":"commands/python-compat/#see-also","title":"See Also","text":"<ul> <li>Getting Started</li> <li>MCP Integration - Use <code>python_compat_check</code> tool in AI assistants</li> </ul>"},{"location":"commands/scan/","title":"scan","text":"<p>Scan your project for deprecated AI library imports and suggest fixes. [BETA]</p>"},{"location":"commands/scan/#usage","title":"Usage","text":"<pre><code>env-doctor scan [PATH]\n</code></pre> <p>If no path is provided, scans the current directory.</p>"},{"location":"commands/scan/#what-it-detects","title":"What It Detects","text":""},{"location":"commands/scan/#langchain-deprecations","title":"LangChain Deprecations","text":"<ul> <li>Old import paths from <code>langchain</code></li> <li>Migration to <code>langchain-core</code>, <code>langchain-community</code>, etc.</li> </ul>"},{"location":"commands/scan/#pydantic-v1-v2","title":"Pydantic V1 \u2192 V2","text":"<ul> <li>Deprecated validator syntax</li> <li>Old import paths</li> </ul>"},{"location":"commands/scan/#other-library-changes","title":"Other Library Changes","text":"<ul> <li>TensorFlow 1.x patterns</li> <li>PyTorch deprecated APIs</li> </ul>"},{"location":"commands/scan/#example-output","title":"Example Output","text":"<pre><code>\ud83d\udd0d SCANNING PROJECT: ./my_project\n\n\ud83d\udcc1 Files scanned: 47\n\ud83d\udcdd Issues found: 3\n\n------------------------------------------------------------\n\nsrc/chains/qa.py:12\n  \u274c Deprecated import: from langchain.chat_models import ChatOpenAI\n  \u2705 Replace with: from langchain_openai import ChatOpenAI\n\nsrc/chains/qa.py:15\n  \u274c Deprecated import: from langchain.embeddings import OpenAIEmbeddings\n  \u2705 Replace with: from langchain_openai import OpenAIEmbeddings\n\nsrc/models/user.py:8\n  \u26a0\ufe0f  Pydantic V1 syntax: @validator\n  \u2705 Replace with: @field_validator (Pydantic V2)\n\nSUMMARY:\n  \u274c Errors:   2\n  \u26a0\ufe0f  Warnings: 1\n</code></pre>"},{"location":"commands/scan/#scan-patterns","title":"Scan Patterns","text":""},{"location":"commands/scan/#langchain","title":"LangChain","text":"Old Import New Import <code>langchain.chat_models</code> <code>langchain_openai</code> or <code>langchain_anthropic</code> <code>langchain.embeddings</code> <code>langchain_openai</code> <code>langchain.vectorstores</code> <code>langchain_community.vectorstores</code> <code>langchain.document_loaders</code> <code>langchain_community.document_loaders</code>"},{"location":"commands/scan/#pydantic","title":"Pydantic","text":"V1 Syntax V2 Syntax <code>@validator</code> <code>@field_validator</code> <code>@root_validator</code> <code>@model_validator</code> <code>class Config:</code> <code>model_config = ConfigDict(...)</code>"},{"location":"commands/scan/#excluding-files","title":"Excluding Files","text":"<p>The scan automatically excludes:</p> <ul> <li><code>.git/</code></li> <li><code>__pycache__/</code></li> <li><code>node_modules/</code></li> <li><code>.venv/</code>, <code>venv/</code></li> <li><code>*.pyc</code>, <code>*.pyo</code></li> </ul>"},{"location":"commands/scan/#advanced-options","title":"Advanced Options","text":""},{"location":"commands/scan/#json-output","title":"JSON Output","text":"<pre><code>env-doctor scan --json\n</code></pre>"},{"location":"commands/scan/#see-also","title":"See Also","text":"<ul> <li>check - Environment diagnosis</li> </ul>"},{"location":"guides/MCP_TESTING/","title":"MCP Tools Testing Guide","text":"<p>This guide explains how to test all 11 env-doctor MCP tools using JSON-RPC.</p>"},{"location":"guides/MCP_TESTING/#available-mcp-tools","title":"Available MCP Tools","text":"# Tool Name Description 1 <code>env_check</code> Full GPU/CUDA environment diagnostics 2 <code>env_check_component</code> Check specific component (driver, CUDA, cuDNN, etc.) 3 <code>python_compat_check</code> Check Python version compatibility with installed AI libraries 4 <code>cuda_info</code> Detailed CUDA toolkit information 5 <code>cudnn_info</code> Detailed cuDNN library information 6 <code>cuda_install</code> Step-by-step CUDA installation instructions 7 <code>install_command</code> Get safe pip install command for AI libraries 8 <code>model_check</code> Check if AI model fits on GPU 9 <code>model_list</code> List all available AI models in database 10 <code>dockerfile_validate</code> Validate Dockerfile for GPU config issues 11 <code>docker_compose_validate</code> Validate docker-compose.yml for GPU config"},{"location":"guides/MCP_TESTING/#testing-methods","title":"Testing Methods","text":""},{"location":"guides/MCP_TESTING/#method-1-automated-test-suite-recommended","title":"Method 1: Automated Test Suite (Recommended)","text":"<p>Run all 11 tools automatically:</p> <pre><code>python tests/test_mcp_tools.py\n</code></pre> <p>This will: - Connect to the MCP server - Test each tool with sample inputs - Display results in a structured format - Show success/failure for each tool</p> <p>Sample Output: <pre><code>================================================================================\nTESTING ENV-DOCTOR MCP TOOLS\n================================================================================\n\n[1/12] Listing available tools...\n\u2713 Found 11 tools:\n  - env_check: Run full GPU/CUDA environment diagnostics. Checks NVIDIA...\n  - env_check_component: Run diagnostics for a specific component. Avail...\n  - python_compat_check: Check Python version compatibility with installed AI...\n  ...\n\n[2/11] Testing env_check...\n\u2713 env_check completed\n  Status: warning\n  Components checked: 5\n  Components detected: 3\n\n[3/11] Testing env_check_component (nvidia_driver)...\n\u2713 env_check_component completed\n  Status: success\n  Detected: True\n  Version: 535.129.03\n</code></pre></p>"},{"location":"guides/MCP_TESTING/#method-2-interactive-testing","title":"Method 2: Interactive Testing","text":"<p>Test tools interactively with custom inputs:</p> <pre><code>python tests/test_mcp_interactive.py\n</code></pre> <p>Interactive Menu: <pre><code>Available tools:\n   1. env_check               - Full environment diagnostics\n   2. env_check_component     - Check specific component\n   3. python_compat_check     - Python version compatibility check\n   4. cuda_info               - Detailed CUDA toolkit info\n   5. cudnn_info              - Detailed cuDNN info\n   6. cuda_install            - CUDA installation guide\n   7. install_command         - Get pip install command\n   8. model_check             - Check if model fits on GPU\n   9. model_list              - List available models\n  10. dockerfile_validate     - Validate Dockerfile\n  11. docker_compose_validate - Validate docker-compose.yml\n\nCommands:\n  - Enter tool number (1-11) to test a tool\n  - Type 'list' to list tools again\n  - Type 'all' to run all tools\n  - Type 'quit' or 'exit' to exit\n</code></pre></p> <p>Test Specific Tool: <pre><code># Test cuda_install with auto-detection\npython tests/test_mcp_interactive.py cuda_install\n\n# Test install_command with custom args\npython tests/test_mcp_interactive.py install_command '{\"library\":\"tensorflow\"}'\n\n# Test model_check\npython tests/test_mcp_interactive.py model_check '{\"model_name\":\"llama-3-70b\",\"precision\":\"int4\"}'\n</code></pre></p>"},{"location":"guides/MCP_TESTING/#method-3-manual-json-rpc-testing","title":"Method 3: Manual JSON-RPC Testing","text":"<p>For advanced users who want to see raw JSON-RPC protocol:</p> <pre><code>bash tests/test_mcp_manual.sh\n</code></pre> <p>Or manually with stdio:</p> <pre><code># Start the MCP server\npython -m env_doctor.mcp.server\n\n# Send JSON-RPC requests (in another terminal or via pipe)\necho '{\"jsonrpc\":\"2.0\",\"id\":1,\"method\":\"tools/list\",\"params\":{}}' | python -m env_doctor.mcp.server\n\necho '{\"jsonrpc\":\"2.0\",\"id\":2,\"method\":\"tools/call\",\"params\":{\"name\":\"cuda_info\",\"arguments\":{}}}' | python -m env_doctor.mcp.server\n\necho '{\"jsonrpc\":\"2.0\",\"id\":3,\"method\":\"tools/call\",\"params\":{\"name\":\"python_compat_check\",\"arguments\":{}}}' | python -m env_doctor.mcp.server\n</code></pre>"},{"location":"guides/MCP_TESTING/#testing-individual-tools","title":"Testing Individual Tools","text":""},{"location":"guides/MCP_TESTING/#1-env_check","title":"1. env_check","text":"<pre><code># Python\nresult = await session.call_tool(\"env_check\", {})\n\n# Expected response\n{\n  \"components\": {\n    \"nvidia_driver\": {...},\n    \"cuda_toolkit\": {...},\n    \"cudnn\": {...}\n  },\n  \"summary\": {\n    \"status\": \"success\",\n    \"component_count\": 5,\n    \"detected_count\": 3\n  }\n}\n</code></pre>"},{"location":"guides/MCP_TESTING/#2-env_check_component","title":"2. env_check_component","text":"<pre><code># Check CUDA toolkit\nresult = await session.call_tool(\"env_check_component\", {\n    \"component\": \"cuda_toolkit\"\n})\n\n# Check NVIDIA driver\nresult = await session.call_tool(\"env_check_component\", {\n    \"component\": \"nvidia_driver\"\n})\n</code></pre>"},{"location":"guides/MCP_TESTING/#3-python_compat_check","title":"3. python_compat_check","text":"<pre><code># Check Python version compatibility with installed AI libraries\nresult = await session.call_tool(\"python_compat_check\", {})\n\n# Expected response\n{\n  \"detected\": true,\n  \"python_version\": \"3.13.0\",\n  \"libraries_checked\": 2,\n  \"conflicts\": [\n    {\n      \"library\": \"torch\",\n      \"installed_version\": \"2.5.1\",\n      \"issue\": \"torch supports Python &lt;=3.12, but you have Python 3.13\",\n      \"note\": \"PyTorch 2.x supports Python 3.9-3.12. Python 3.13 support experimental.\"\n    },\n    {\n      \"library\": \"tensorflow\",\n      \"installed_version\": \"2.15.0\",\n      \"issue\": \"tensorflow supports Python &lt;=3.12, but you have Python 3.13\",\n      \"note\": \"TensorFlow 2.15+ requires Python 3.9-3.12. Python 3.13 not yet supported.\"\n    }\n  ],\n  \"cascades\": [\n    {\n      \"library\": \"torch\",\n      \"severity\": \"high\",\n      \"description\": \"PyTorch's Python version constraint affects all torch ecosystem packages\",\n      \"affected\": [\"torchvision\", \"torchaudio\", \"triton\"]\n    }\n  ],\n  \"issues\": [\n    \"torch supports Python &lt;=3.12, but you have Python 3.13\",\n    \"tensorflow supports Python &lt;=3.12, but you have Python 3.13\"\n  ],\n  \"recommendations\": [\n    \"Consider using Python 3.12 or lower for full compatibility\",\n    \"Cascade: torch constraint also affects: torchvision, torchaudio, triton\"\n  ]\n}\n</code></pre>"},{"location":"guides/MCP_TESTING/#4-cuda_info","title":"4. cuda_info","text":"<pre><code># Get detailed CUDA info\nresult = await session.call_tool(\"cuda_info\", {})\n\n# Response includes\n{\n  \"detected\": true,\n  \"version\": \"12.1\",\n  \"metadata\": {\n    \"installations\": [...],\n    \"nvcc\": {...},\n    \"cuda_home\": {...},\n    \"driver_compatibility\": {...}\n  }\n}\n</code></pre>"},{"location":"guides/MCP_TESTING/#5-cudnn_info","title":"5. cudnn_info","text":"<pre><code># Get detailed cuDNN info\nresult = await session.call_tool(\"cudnn_info\", {})\n\n# Response\n{\n  \"detected\": true,\n  \"version\": \"8.9.0\",\n  \"path\": \"/usr/lib/x86_64-linux-gnu/libcudnn.so.8\"\n}\n</code></pre>"},{"location":"guides/MCP_TESTING/#6-cuda_install","title":"6. cuda_install","text":"<pre><code># Auto-detect best CUDA version from driver\nresult = await session.call_tool(\"cuda_install\", {})\n\n# Specify CUDA version\nresult = await session.call_tool(\"cuda_install\", {\n    \"version\": \"12.4\"\n})\n\n# Response includes\n{\n  \"platform\": {\n    \"os\": \"linux\",\n    \"distro\": \"ubuntu\",\n    \"distro_version\": \"22.04\",\n    \"arch\": \"x86_64\",\n    \"is_wsl2\": false\n  },\n  \"recommended_version\": \"12.4\",\n  \"install_info\": {\n    \"label\": \"Ubuntu 22.04 (x86_64) - Network Install\",\n    \"steps\": [...],\n    \"post_install\": [...],\n    \"verify\": \"nvcc --version\"\n  }\n}\n</code></pre>"},{"location":"guides/MCP_TESTING/#7-install_command","title":"7. install_command","text":"<pre><code># Get install command for PyTorch\nresult = await session.call_tool(\"install_command\", {\n    \"library\": \"torch\"\n})\n\n# Get install command for TensorFlow\nresult = await session.call_tool(\"install_command\", {\n    \"library\": \"tensorflow\"\n})\n\n# Response\n{\n  \"library\": \"torch\",\n  \"driver_detected\": true,\n  \"driver_version\": \"535.129.03\",\n  \"max_cuda\": \"12.2\",\n  \"install_command\": \"pip install torch==2.5.1 torchvision==0.20.1...\"\n}\n</code></pre>"},{"location":"guides/MCP_TESTING/#8-model_check","title":"8. model_check","text":"<pre><code># Check if llama-3-8b fits\nresult = await session.call_tool(\"model_check\", {\n    \"model_name\": \"llama-3-8b\"\n})\n\n# Check specific precision\nresult = await session.call_tool(\"model_check\", {\n    \"model_name\": \"stable-diffusion-xl\",\n    \"precision\": \"fp16\"\n})\n\n# Response\n{\n  \"success\": true,\n  \"model_name\": \"llama-3-8b\",\n  \"gpu_info\": {...},\n  \"vram_requirements\": {\n    \"fp16\": {\"vram_mb\": 19200},\n    \"int4\": {\"vram_mb\": 4800}\n  },\n  \"compatibility\": {...}\n}\n</code></pre>"},{"location":"guides/MCP_TESTING/#9-model_list","title":"9. model_list","text":"<pre><code># List all models\nresult = await session.call_tool(\"model_list\", {})\n\n# Response\n{\n  \"models_by_category\": {\n    \"llm\": [...],\n    \"diffusion\": [...],\n    \"audio\": [...]\n  },\n  \"stats\": {\n    \"total_models\": 60,\n    \"categories\": 4\n  }\n}\n</code></pre>"},{"location":"guides/MCP_TESTING/#10-dockerfile_validate","title":"10. dockerfile_validate","text":"<pre><code># Validate Dockerfile content\ndockerfile = \"\"\"\nFROM python:3.10\nRUN pip install torch torchvision torchaudio\nCMD [\"python\", \"app.py\"]\n\"\"\"\n\nresult = await session.call_tool(\"dockerfile_validate\", {\n    \"content\": dockerfile\n})\n\n# Response\n{\n  \"success\": false,\n  \"error_count\": 1,\n  \"warning_count\": 2,\n  \"issues\": [\n    {\n      \"line_number\": 2,\n      \"severity\": \"error\",\n      \"issue\": \"Missing --index-url for PyTorch\",\n      \"recommendation\": \"Add CUDA-specific index URL\"\n    }\n  ]\n}\n</code></pre>"},{"location":"guides/MCP_TESTING/#11-docker_compose_validate","title":"11. docker_compose_validate","text":"<pre><code># Validate docker-compose.yml\ncompose = \"\"\"\nversion: '3.8'\nservices:\n  app:\n    image: pytorch/pytorch:2.0.0-cuda11.7-cudnn8-runtime\n    command: python train.py\n\"\"\"\n\nresult = await session.call_tool(\"docker_compose_validate\", {\n    \"content\": compose\n})\n\n# Response\n{\n  \"success\": false,\n  \"error_count\": 1,\n  \"warning_count\": 0,\n  \"issues\": [\n    {\n      \"severity\": \"error\",\n      \"issue\": \"Missing GPU device configuration\",\n      \"recommendation\": \"Add deploy.resources.reservations.devices\"\n    }\n  ]\n}\n</code></pre>"},{"location":"guides/MCP_TESTING/#integration-with-claude-desktop","title":"Integration with Claude Desktop","text":"<p>Add to your <code>claude_desktop_config.json</code>:</p> <pre><code>{\n  \"mcpServers\": {\n    \"env-doctor\": {\n      \"command\": \"python\",\n      \"args\": [\"-m\", \"env_doctor.mcp.server\"]\n    }\n  }\n}\n</code></pre> <p>Or if installed globally: <pre><code>{\n  \"mcpServers\": {\n    \"env-doctor\": {\n      \"command\": \"env-doctor-mcp\"\n    }\n  }\n}\n</code></pre></p>"},{"location":"guides/MCP_TESTING/#common-test-scenarios","title":"Common Test Scenarios","text":""},{"location":"guides/MCP_TESTING/#scenario-1-new-gpu-setup","title":"Scenario 1: New GPU Setup","text":"<pre><code># 1. Check driver\nawait session.call_tool(\"env_check_component\", {\"component\": \"nvidia_driver\"})\n\n# 2. Check CUDA\nawait session.call_tool(\"cuda_info\", {})\n\n# 3. Get CUDA install instructions\nawait session.call_tool(\"cuda_install\", {})\n\n# 4. Get PyTorch install command\nawait session.call_tool(\"install_command\", {\"library\": \"torch\"})\n</code></pre>"},{"location":"guides/MCP_TESTING/#scenario-2-model-deployment","title":"Scenario 2: Model Deployment","text":"<pre><code># 1. Check hardware\nawait session.call_tool(\"env_check\", {})\n\n# 2. Check if model fits\nawait session.call_tool(\"model_check\", {\"model_name\": \"llama-3-70b\"})\n\n# 3. Validate Dockerfile\nawait session.call_tool(\"dockerfile_validate\", {\"content\": dockerfile})\n\n# 4. Validate docker-compose\nawait session.call_tool(\"docker_compose_validate\", {\"content\": compose})\n</code></pre>"},{"location":"guides/MCP_TESTING/#scenario-3-python-version-compatibility-check","title":"Scenario 3: Python Version Compatibility Check","text":"<pre><code># 1. Check Python version against installed libraries\nawait session.call_tool(\"python_compat_check\", {})\n\n# 2. If conflicts found, get the safe install command for a compatible version\nawait session.call_tool(\"install_command\", {\"library\": \"torch\"})\n</code></pre>"},{"location":"guides/MCP_TESTING/#scenario-4-debugging-installation-issues","title":"Scenario 4: Debugging Installation Issues","text":"<pre><code># 1. Full environment check\nawait session.call_tool(\"env_check\", {})\n\n# 2. Detailed CUDA analysis\nawait session.call_tool(\"cuda_info\", {})\n\n# 3. Detailed cuDNN analysis\nawait session.call_tool(\"cudnn_info\", {})\n\n# 4. Get correct install commands\nawait session.call_tool(\"install_command\", {\"library\": \"torch\"})\n</code></pre>"},{"location":"guides/MCP_TESTING/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/MCP_TESTING/#server-not-starting","title":"Server Not Starting","text":"<pre><code># Check if env-doctor is installed\npython -m env_doctor.mcp.server\n\n# Check dependencies\npip list | grep mcp\n</code></pre>"},{"location":"guides/MCP_TESTING/#tool-errors","title":"Tool Errors","text":"<p>Check the error message in the response: <pre><code>{\n  \"error\": \"No NVIDIA driver detected\",\n  \"driver_download\": \"https://www.nvidia.com/Download/index.aspx\"\n}\n</code></pre></p>"},{"location":"guides/MCP_TESTING/#invalid-arguments","title":"Invalid Arguments","text":"<p>Each tool has specific required/optional arguments. Check the tool definition: <pre><code>tools = await session.list_tools()\nfor tool in tools.tools:\n    print(f\"{tool.name}: {tool.inputSchema}\")\n</code></pre></p>"},{"location":"guides/MCP_TESTING/#performance-notes","title":"Performance Notes","text":"<ul> <li><code>env_check</code> scans all components (~1-2 seconds)</li> <li><code>python_compat_check</code> scans installed libraries against compatibility matrix (~0.3 seconds)</li> <li><code>cuda_info</code> and <code>cudnn_info</code> are fast (&lt;0.5 seconds)</li> <li><code>model_check</code> may fetch from HuggingFace API first time (cached afterward)</li> <li><code>dockerfile_validate</code> and <code>docker_compose_validate</code> are instant</li> <li><code>cuda_install</code> detects platform and looks up instructions (~0.2 seconds)</li> </ul>"},{"location":"guides/ci-cd/","title":"CI/CD Integration","text":"<p>Env-Doctor supports JSON output and proper exit codes for seamless CI/CD integration.</p>"},{"location":"guides/ci-cd/#quick-start","title":"Quick Start","text":"<pre><code># GitHub Actions\n- run: pip install env-doctor\n- run: env-doctor check --ci\n</code></pre>"},{"location":"guides/ci-cd/#exit-codes","title":"Exit Codes","text":"<p>When using <code>--ci</code> flag:</p> Code Meaning Action <code>0</code> All checks passed Continue pipeline <code>1</code> Warnings or non-critical issues Review, may continue <code>2</code> Critical errors detected Fail pipeline"},{"location":"guides/ci-cd/#github-actions","title":"GitHub Actions","text":""},{"location":"guides/ci-cd/#basic-validation","title":"Basic Validation","text":"<pre><code>name: Validate Environment\non: [push, pull_request]\n\njobs:\n  validate:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - uses: actions/setup-python@v5\n        with:\n          python-version: '3.10'\n      - run: pip install env-doctor\n      - run: env-doctor check --ci\n</code></pre>"},{"location":"guides/ci-cd/#gpu-runner-validation","title":"GPU Runner Validation","text":"<p>For self-hosted GPU runners:</p> <pre><code>name: GPU Environment Check\non: [push]\n\njobs:\n  gpu-check:\n    runs-on: [self-hosted, gpu]\n    steps:\n      - uses: actions/checkout@v4\n      - run: pip install env-doctor\n      - name: Validate GPU Environment\n        run: env-doctor check --ci\n      - name: Check Model Compatibility\n        run: env-doctor model llama-3-8b --json\n</code></pre>"},{"location":"guides/ci-cd/#dockerfile-validation","title":"Dockerfile Validation","text":"<pre><code>name: Validate Docker\non:\n  push:\n    paths:\n      - 'Dockerfile'\n      - 'docker-compose.yml'\n\njobs:\n  validate:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - run: pip install env-doctor\n      - name: Validate Dockerfile\n        run: env-doctor dockerfile --ci\n      - name: Validate Docker Compose\n        run: env-doctor docker-compose --ci\n</code></pre>"},{"location":"guides/ci-cd/#gitlab-ci","title":"GitLab CI","text":"<pre><code>stages:\n  - validate\n\nvalidate-environment:\n  stage: validate\n  image: python:3.10\n  script:\n    - pip install env-doctor\n    - env-doctor check --ci\n  rules:\n    - if: $CI_PIPELINE_SOURCE == \"merge_request_event\"\n</code></pre>"},{"location":"guides/ci-cd/#json-output","title":"JSON Output","text":"<p>For custom parsing and reporting:</p> <pre><code>env-doctor check --json\n</code></pre>"},{"location":"guides/ci-cd/#parsing-in-bash","title":"Parsing in Bash","text":"<pre><code>#!/bin/bash\n\n# Run check and capture output\nRESULT=$(env-doctor check --json)\n\n# Extract specific fields with jq\nSTATUS=$(echo \"$RESULT\" | jq -r '.status')\nDRIVER_VERSION=$(echo \"$RESULT\" | jq -r '.checks.driver.version')\nCUDA_VERSION=$(echo \"$RESULT\" | jq -r '.checks.cuda.version')\n\necho \"Status: $STATUS\"\necho \"Driver: $DRIVER_VERSION\"\necho \"CUDA: $CUDA_VERSION\"\n\n# Conditional logic\nif [ \"$STATUS\" = \"error\" ]; then\n  echo \"Environment check failed!\"\n  exit 1\nfi\n\n# Detect hard vs soft GPU architecture mismatch\nARCH_STATUS=$(echo \"$RESULT\" | jq -r '.checks.compute_compatibility.status // empty')\nCUDA_AVAILABLE=$(echo \"$RESULT\" | jq -r '.checks.compute_compatibility.cuda_available // empty')\n\nif [ \"$ARCH_STATUS\" = \"mismatch\" ]; then\n  if [ \"$CUDA_AVAILABLE\" = \"false\" ]; then\n    echo \"Hard mismatch: torch.cuda.is_available() is False \u2014 install PyTorch nightly\"\n    exit 1\n  elif [ \"$CUDA_AVAILABLE\" = \"true\" ]; then\n    echo \"Soft mismatch: CUDA works via PTX JIT but performance may degrade\"\n  fi\nfi\n</code></pre>"},{"location":"guides/ci-cd/#parsing-in-python","title":"Parsing in Python","text":"<pre><code>import json\nimport subprocess\n\nresult = subprocess.run(\n    [\"env-doctor\", \"check\", \"--json\"],\n    capture_output=True,\n    text=True\n)\ndata = json.loads(result.stdout)\n\n# Access check results\nif data[\"status\"] == \"success\":\n    print(f\"Driver: {data['checks']['driver']['version']}\")\n    print(f\"CUDA: {data['checks']['cuda']['version']}\")\nelse:\n    print(f\"Issues found: {data['summary']['issues_count']}\")\n</code></pre>"},{"location":"guides/ci-cd/#conditional-installation","title":"Conditional Installation","text":"<p>Install GPU or CPU packages based on environment:</p> <pre><code>#!/bin/bash\n\n# Check if GPU is available\nif env-doctor check --json | jq -e '.checks.driver.detected' &gt; /dev/null; then\n  echo \"GPU detected, installing CUDA version\"\n  pip install torch torchvision\nelse\n  echo \"No GPU, installing CPU version\"\n  pip install torch torchvision --index-url https://download.pytorch.org/whl/cpu\nfi\n</code></pre>"},{"location":"guides/ci-cd/#pre-commit-hook","title":"Pre-commit Hook","text":"<p>Validate Dockerfiles before commit:</p> <pre><code># .pre-commit-config.yaml\nrepos:\n  - repo: local\n    hooks:\n      - id: validate-dockerfile\n        name: Validate Dockerfile\n        entry: env-doctor dockerfile --ci\n        language: system\n        files: Dockerfile\n        pass_filenames: false\n</code></pre>"},{"location":"guides/ci-cd/#monitoring-integration","title":"Monitoring Integration","text":"<p>Store results for tracking over time:</p> <pre><code>import json\nimport subprocess\nfrom datetime import datetime\n\ndef collect_env_metrics():\n    result = subprocess.run(\n        [\"env-doctor\", \"check\", \"--json\"],\n        capture_output=True,\n        text=True\n    )\n    data = json.loads(result.stdout)\n\n    return {\n        \"timestamp\": datetime.utcnow().isoformat(),\n        \"status\": data[\"status\"],\n        \"driver_version\": data[\"checks\"][\"driver\"].get(\"version\"),\n        \"cuda_version\": data[\"checks\"][\"cuda\"].get(\"version\"),\n        \"issues_count\": data[\"summary\"][\"issues_count\"]\n    }\n\n# Send to your monitoring system\nmetrics = collect_env_metrics()\n# monitoring_client.send(metrics)\n</code></pre>"},{"location":"guides/ci-cd/#see-also","title":"See Also","text":"<ul> <li>check Command - Full command reference</li> <li>GitHub Actions Example</li> </ul>"},{"location":"guides/mcp-integration/","title":"MCP Server Integration","text":"<p>Env-Doctor includes a built-in Model Context Protocol (MCP) server that exposes its diagnostic capabilities as read-only tools for AI assistants like Claude Code, Claude Desktop, Zed, and other MCP-compatible clients.</p>"},{"location":"guides/mcp-integration/#what-is-mcp","title":"What is MCP?","text":"<p>The Model Context Protocol is an open standard that enables AI assistants to securely access external tools and data sources. The env-doctor MCP server allows AI assistants to:</p> <ul> <li>Diagnose GPU/CUDA environments</li> <li>Check AI model compatibility</li> <li>Validate Dockerfiles for GPU issues</li> <li>List available models and their requirements</li> </ul> <p>All operations are read-only - the MCP server cannot modify your system.</p>"},{"location":"guides/mcp-integration/#quick-start","title":"Quick Start","text":""},{"location":"guides/mcp-integration/#1-install-env-doctor","title":"1. Install env-doctor","text":"<pre><code>pip install env-doctor\n</code></pre>"},{"location":"guides/mcp-integration/#2-configure-claude-desktop","title":"2. Configure Claude Desktop","text":"<p>Add the env-doctor MCP server to your Claude Desktop configuration:</p> <p>macOS/Linux: Edit <code>~/Library/Application Support/Claude/claude_desktop_config.json</code></p> <p>Windows: Edit <code>%APPDATA%\\Claude\\claude_desktop_config.json</code></p> <pre><code>{\n  \"mcpServers\": {\n    \"env-doctor\": {\n      \"command\": \"env-doctor-mcp\"\n    }\n  }\n}\n</code></pre>"},{"location":"guides/mcp-integration/#3-restart-claude-desktop","title":"3. Restart Claude Desktop","text":"<p>After saving the configuration, restart Claude Desktop. The env-doctor tools will be available automatically.</p>"},{"location":"guides/mcp-integration/#available-tools","title":"Available Tools","text":"<p>The MCP server exposes 11 diagnostic tools:</p>"},{"location":"guides/mcp-integration/#python_compat_check","title":"<code>python_compat_check</code>","text":"<p>Check Python version compatibility with installed AI libraries.</p> <p>What it checks: - Current Python version against each library's supported range - Libraries with version conflicts (PyTorch, TensorFlow, JAX, etc.) - Dependency cascades \u2014 where one library's Python ceiling forces limits on downstream packages (e.g. <code>torch</code> \u2192 <code>torchvision</code>, <code>torchaudio</code>, <code>triton</code>)</p> <p>Returns: Python version, list of conflicts with notes, cascade impacts, and recommendations.</p> <p>Example prompt:</p> <p>\"Is my Python version compatible with my installed AI libraries?\"</p>"},{"location":"guides/mcp-integration/#env_check","title":"<code>env_check</code>","text":"<p>Run full GPU/CUDA environment diagnostics.</p> <p>What it checks: - NVIDIA driver version and status - CUDA toolkit installation and configuration - cuDNN library detection - Python AI libraries (PyTorch, TensorFlow, JAX) - WSL2 GPU forwarding (Windows)</p> <p>Returns: Status, versions, issues, and recommendations for each component.</p> <p>Example prompt:</p> <p>\"Check my GPU environment\"</p>"},{"location":"guides/mcp-integration/#env_check_component","title":"<code>env_check_component</code>","text":"<p>Run diagnostics for a specific component.</p> <p>Parameters: - <code>component</code> (required): One of:   - <code>nvidia_driver</code> - GPU driver detection   - <code>cuda_toolkit</code> - CUDA installation analysis   - <code>cudnn</code> - cuDNN library detection   - <code>python_library</code> - AI library compatibility   - <code>wsl2</code> - WSL2 GPU forwarding validation</p> <p>Returns: Detailed status for the specified component.</p> <p>Example prompt:</p> <p>\"Check my CUDA toolkit configuration\"</p>"},{"location":"guides/mcp-integration/#model_check","title":"<code>model_check</code>","text":"<p>Check if an AI model fits on available GPU hardware.</p> <p>Parameters: - <code>model_name</code> (required): Model name or HuggingFace ID   - Examples: <code>llama-3-8b</code>, <code>meta-llama/Llama-2-7b-hf</code>, <code>stable-diffusion-xl</code> - <code>precision</code> (optional): Specific precision to check   - Options: <code>fp32</code>, <code>fp16</code>, <code>bf16</code>, <code>int8</code>, <code>int4</code>, <code>fp8</code></p> <p>Returns: VRAM requirements, compatibility analysis, and recommendations.</p> <p>Example prompts:</p> <p>\"Can I run Llama 3 8B on my GPU?\" \"Check if stable-diffusion-xl fits in fp16\"</p>"},{"location":"guides/mcp-integration/#model_list","title":"<code>model_list</code>","text":"<p>List all available AI models in the database.</p> <p>Returns: Models grouped by category (LLM, diffusion, audio, vision, multimodal) with parameter counts and HuggingFace IDs.</p> <p>Example prompt:</p> <p>\"What models are available in the database?\"</p>"},{"location":"guides/mcp-integration/#dockerfile_validate","title":"<code>dockerfile_validate</code>","text":"<p>Validate Dockerfile content for GPU/CUDA configuration issues.</p> <p>Parameters: - <code>content</code> (required): Dockerfile content as a string</p> <p>What it checks: - CPU-only base images with GPU libraries - Missing PyTorch <code>--index-url</code> flags - CUDA version mismatches - Driver installations in containers (anti-pattern) - Deprecated package usage - Runtime vs devel base image mismatches</p> <p>Returns: List of issues with line numbers, severity, and corrected commands.</p> <p>Example prompt:</p> <p>\"Validate this Dockerfile: [paste Dockerfile content]\"</p>"},{"location":"guides/mcp-integration/#docker_compose_validate","title":"<code>docker_compose_validate</code>","text":"<p>Validate docker-compose.yml content for GPU configuration issues.</p> <p>Parameters: - <code>content</code> (required): docker-compose.yml content as a string</p> <p>What it checks: - Missing GPU runtime configuration - Incorrect device mappings - Missing environment variables - Volume mount issues - GPU resource allocation</p> <p>Returns: List of issues with severity levels and corrected configuration.</p> <p>Example prompt:</p> <p>\"Validate this docker-compose.yml: [paste content]\"</p>"},{"location":"guides/mcp-integration/#install_command","title":"<code>install_command</code>","text":"<p>Get safe installation command for a library based on detected GPU driver.</p> <p>Parameters: - <code>library</code> (required): Library name (e.g., \"torch\", \"tensorflow\", \"jax\")</p> <p>Returns: - Detected driver version and max CUDA support - Safe pip install command with correct CUDA version</p> <p>Example prompts:</p> <p>\"What's the safe install command for PyTorch?\" \"How do I install TensorFlow for my GPU?\"</p>"},{"location":"guides/mcp-integration/#cuda_info","title":"<code>cuda_info</code>","text":"<p>Get detailed CUDA toolkit information and diagnostics.</p> <p>Returns: - All CUDA installations found on system - nvcc compiler information - CUDA_HOME environment variable status - Runtime library locations - PATH and LD_LIBRARY_PATH configuration - Driver compatibility analysis - Detected issues and recommendations</p> <p>Example prompt:</p> <p>\"Show me detailed CUDA toolkit information\"</p>"},{"location":"guides/mcp-integration/#cudnn_info","title":"<code>cudnn_info</code>","text":"<p>Get detailed cuDNN library information and diagnostics.</p> <p>Returns: - cuDNN version and installation path - All cuDNN libraries found - Symlink status (Linux) - PATH configuration (Windows) - CUDA compatibility analysis - Detected issues and recommendations</p> <p>Example prompt:</p> <p>\"Check my cuDNN installation\"</p>"},{"location":"guides/mcp-integration/#cuda_install","title":"<code>cuda_install</code>","text":"<p>Get step-by-step CUDA Toolkit installation instructions for your platform.</p> <p>Parameters: - <code>version</code> (optional): Specific CUDA version to install (auto-detects from driver if not specified)</p> <p>Returns: - Platform detection (OS, distro, architecture) - Recommended CUDA version based on driver - Platform-specific installation steps - Official download links</p> <p>Example prompts:</p> <p>\"How do I install CUDA Toolkit?\" \"Show me CUDA 12.1 installation steps\"</p>"},{"location":"guides/mcp-integration/#configuration-options","title":"Configuration Options","text":""},{"location":"guides/mcp-integration/#basic-configuration","title":"Basic Configuration","text":"<p>Minimal setup for Claude Desktop:</p> <pre><code>{\n  \"mcpServers\": {\n    \"env-doctor\": {\n      \"command\": \"env-doctor-mcp\"\n    }\n  }\n}\n</code></pre>"},{"location":"guides/mcp-integration/#python-virtual-environment","title":"Python Virtual Environment","text":"<p>If env-doctor is installed in a virtual environment:</p> <pre><code>{\n  \"mcpServers\": {\n    \"env-doctor\": {\n      \"command\": \"/path/to/venv/bin/env-doctor-mcp\"\n    }\n  }\n}\n</code></pre> <p>macOS/Linux: <pre><code>{\n  \"mcpServers\": {\n    \"env-doctor\": {\n      \"command\": \"/home/user/.venv/bin/env-doctor-mcp\"\n    }\n  }\n}\n</code></pre></p> <p>Windows: <pre><code>{\n  \"mcpServers\": {\n    \"env-doctor\": {\n      \"command\": \"C:\\\\Users\\\\user\\\\.venv\\\\Scripts\\\\env-doctor-mcp.exe\"\n    }\n  }\n}\n</code></pre></p>"},{"location":"guides/mcp-integration/#multiple-python-versions","title":"Multiple Python Versions","text":"<p>If you have multiple Python versions, specify the full path:</p> <pre><code>{\n  \"mcpServers\": {\n    \"env-doctor\": {\n      \"command\": \"python3.10\",\n      \"args\": [\"-m\", \"env_doctor.mcp\"]\n    }\n  }\n}\n</code></pre>"},{"location":"guides/mcp-integration/#usage-examples","title":"Usage Examples","text":""},{"location":"guides/mcp-integration/#diagnosing-environment-issues","title":"Diagnosing Environment Issues","text":"<p>User: My PyTorch isn't detecting my GPU. Can you help?</p> <p>Claude with env-doctor MCP: 1. Runs <code>env_check</code> to diagnose the full environment 2. Identifies version mismatches 3. Provides specific fix commands</p>"},{"location":"guides/mcp-integration/#checking-model-compatibility","title":"Checking Model Compatibility","text":"<p>User: I want to run Llama 2 70B. Will it fit on my 2x RTX 3090?</p> <p>Claude with env-doctor MCP: 1. Runs <code>model_check</code> for <code>llama-2-70b</code> 2. Analyzes VRAM requirements across precisions 3. Recommends optimal precision (e.g., int4) or multi-GPU setup</p>"},{"location":"guides/mcp-integration/#validating-dockerfiles","title":"Validating Dockerfiles","text":"<p>User: Here's my Dockerfile for a PyTorch project. Are there any issues?</p> <pre><code>FROM python:3.10\nRUN pip install torch torchvision\n</code></pre> <p>Claude with env-doctor MCP: 1. Runs <code>dockerfile_validate</code> on the content 2. Detects CPU-only base image 3. Identifies missing <code>--index-url</code> flag 4. Provides corrected Dockerfile with GPU-enabled base image</p>"},{"location":"guides/mcp-integration/#listing-available-models","title":"Listing Available Models","text":"<p>User: What AI models can I check compatibility for?</p> <p>Claude with env-doctor MCP: 1. Runs <code>model_list</code> 2. Shows models grouped by category (LLMs, diffusion, audio, etc.) 3. Includes parameter counts and HuggingFace IDs</p>"},{"location":"guides/mcp-integration/#supported-mcp-clients","title":"Supported MCP Clients","text":"<p>The env-doctor MCP server is compatible with:</p> <ul> <li>Claude Desktop (macOS, Windows, Linux)</li> <li>Zed Editor (via MCP support)</li> <li>Any MCP-compatible client using stdio transport</li> </ul>"},{"location":"guides/mcp-integration/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/mcp-integration/#server-not-appearing-in-claude-desktop","title":"Server Not Appearing in Claude Desktop","text":"<ol> <li> <p>Verify installation: <pre><code>env-doctor-mcp --help\n</code></pre></p> </li> <li> <p>Check configuration file location:</p> </li> <li>macOS: <code>~/Library/Application Support/Claude/claude_desktop_config.json</code></li> <li>Windows: <code>%APPDATA%\\Claude\\claude_desktop_config.json</code></li> <li> <p>Linux: <code>~/.config/Claude/claude_desktop_config.json</code></p> </li> <li> <p>Validate JSON syntax:    Use a JSON validator to ensure your config file is valid.</p> </li> <li> <p>Check Claude Desktop logs:</p> </li> <li>macOS: <code>~/Library/Logs/Claude/</code></li> <li> <p>Windows: <code>%APPDATA%\\Claude\\logs\\</code></p> </li> <li> <p>Restart Claude Desktop after making config changes.</p> </li> </ol>"},{"location":"guides/mcp-integration/#permission-errors","title":"Permission Errors","text":"<p>If you see permission errors, ensure env-doctor-mcp is executable:</p> <pre><code># macOS/Linux\nchmod +x $(which env-doctor-mcp)\n</code></pre>"},{"location":"guides/mcp-integration/#virtual-environment-issues","title":"Virtual Environment Issues","text":"<p>If using a virtual environment, use the full path to the env-doctor-mcp executable:</p> <pre><code># Find the path\nwhich env-doctor-mcp\n\n# Or\npython -c \"import sys; print(sys.prefix + '/bin/env-doctor-mcp')\"\n</code></pre>"},{"location":"guides/mcp-integration/#security-considerations","title":"Security Considerations","text":"<p>The env-doctor MCP server is designed with security in mind:</p> <ul> <li>Read-only operations: Cannot modify system configuration</li> <li>No network access: Doesn't make external API calls (except optional HuggingFace model lookups)</li> <li>Local execution: Runs entirely on your machine</li> <li>No data collection: Doesn't send diagnostic data anywhere</li> </ul> <p>The only write operation is caching HuggingFace model metadata locally to <code>~/.env_doctor_cache.json</code>.</p>"},{"location":"guides/mcp-integration/#advanced-usage","title":"Advanced Usage","text":""},{"location":"guides/mcp-integration/#programmatic-access","title":"Programmatic Access","text":"<p>You can also use the MCP server programmatically in Python:</p> <pre><code>from env_doctor.mcp import tools\n\n# Check full environment\nresult = tools.env_check()\nprint(result)\n\n# Check specific component\nresult = tools.env_check_component(\"nvidia_driver\")\nprint(result)\n\n# Check model compatibility\nresult = tools.model_check(\"llama-3-8b\")\nprint(result)\n\n# List available models\nresult = tools.model_list()\nprint(result)\n\n# Validate Dockerfile\ndockerfile_content = \"\"\"\nFROM nvidia/cuda:12.1.0-runtime-ubuntu22.04\nRUN pip install torch --index-url https://download.pytorch.org/whl/cu121\n\"\"\"\nresult = tools.dockerfile_validate(dockerfile_content)\nprint(result)\n</code></pre>"},{"location":"guides/mcp-integration/#custom-mcp-clients","title":"Custom MCP Clients","text":"<p>To integrate env-doctor with a custom MCP client:</p> <pre><code>from mcp import StdioServerParameters\nfrom mcp.client.stdio import stdio_client\n\n# Connect to env-doctor MCP server\nserver_params = StdioServerParameters(\n    command=\"env-doctor-mcp\",\n    args=[]\n)\n\nasync with stdio_client(server_params) as (read, write):\n    # Use the client\n    pass\n</code></pre>"},{"location":"guides/mcp-integration/#see-also","title":"See Also","text":"<ul> <li>Model Context Protocol Documentation</li> <li>Claude Desktop Setup</li> <li>check Command - CLI equivalent of env_check</li> <li>model Command - CLI equivalent of model_check</li> <li>dockerfile Command - CLI equivalent of dockerfile_validate</li> </ul>"},{"location":"guides/recording-demos/","title":"Recording Demos","text":"<p>This guide explains how to record terminal demos for the documentation.</p>"},{"location":"guides/recording-demos/#asciinema-recommended","title":"Asciinema (Recommended)","text":"<p>Asciinema creates lightweight, copy-pasteable terminal recordings.</p>"},{"location":"guides/recording-demos/#installation","title":"Installation","text":"<pre><code># macOS\nbrew install asciinema\n\n# Ubuntu/Debian\nsudo apt install asciinema\n\n# pip\npip install asciinema\n</code></pre>"},{"location":"guides/recording-demos/#recording","title":"Recording","text":"<pre><code># Start recording\nasciinema rec demo.cast\n\n# Run your commands...\nenv-doctor check\nenv-doctor model llama-3-8b\n\n# Press Ctrl+D or type 'exit' to stop\n</code></pre>"},{"location":"guides/recording-demos/#tips-for-good-recordings","title":"Tips for Good Recordings","text":"<ol> <li>Clear screen first: Start with <code>clear</code></li> <li>Type slowly: Pause between commands for readability</li> <li>Keep it short: 15-30 seconds is ideal</li> <li>Show one feature: Focus on a single command per recording</li> </ol>"},{"location":"guides/recording-demos/#uploading","title":"Uploading","text":"<pre><code># Upload to asciinema.org\nasciinema upload demo.cast\n\n# Or authenticate first for your account\nasciinema auth\nasciinema upload demo.cast\n</code></pre>"},{"location":"guides/recording-demos/#embedding-in-docs","title":"Embedding in Docs","text":"<p>After uploading, you'll get a URL like <code>https://asciinema.org/a/123456</code>.</p> <p>In Markdown:</p> <pre><code>[![asciicast](https://asciinema.org/a/123456.svg)](https://asciinema.org/a/123456)\n</code></pre> <p>As JavaScript embed (auto-plays):</p> <pre><code>&lt;script src=\"https://asciinema.org/a/123456.js\" id=\"asciicast-123456\" async data-autoplay=\"true\" data-speed=\"1.5\"&gt;&lt;/script&gt;\n</code></pre>"},{"location":"guides/recording-demos/#recording-checklist","title":"Recording Checklist","text":"<p>Before recording:</p> <ul> <li>[ ] Clean terminal (no sensitive info)</li> <li>[ ] Clear history if needed</li> <li>[ ] Set consistent terminal size (80x24 recommended)</li> <li>[ ] Prepare commands to run</li> </ul> <p>Recording:</p> <ul> <li>[ ] Clear screen</li> <li>[ ] Type commands clearly</li> <li>[ ] Pause to let output be readable</li> <li>[ ] Keep under 30 seconds</li> </ul> <p>After recording:</p> <ul> <li>[ ] Review the recording</li> <li>[ ] Re-record if needed</li> <li>[ ] Upload and get embed code</li> </ul>"},{"location":"guides/wsl2/","title":"WSL2 GPU Support","text":"<p>Env-Doctor provides comprehensive WSL2 environment detection and GPU forwarding validation.</p>"},{"location":"guides/wsl2/#environment-detection","title":"Environment Detection","text":"<p>Env-Doctor automatically detects your environment:</p> Environment GPU Support Native Linux Full CUDA support WSL1 No GPU support (CUDA not available) WSL2 GPU forwarding from Windows host"},{"location":"guides/wsl2/#how-wsl2-gpu-works","title":"How WSL2 GPU Works","text":"<p>In WSL2, GPU access is forwarded from the Windows host:</p> <pre><code>Windows Host (NVIDIA Driver) \u2192 WSL2 (CUDA libraries) \u2192 Your Application\n</code></pre> <p>Key points:</p> <ul> <li>NVIDIA driver is installed on Windows only</li> <li>WSL2 uses special libraries in <code>/usr/lib/wsl/lib/</code></li> <li>Do NOT install NVIDIA drivers inside WSL2</li> </ul>"},{"location":"guides/wsl2/#wsl2-validation","title":"WSL2 Validation","text":"<p>Run <code>env-doctor check</code> to validate your WSL2 GPU setup:</p> <pre><code>env-doctor check\n</code></pre> <p>Healthy WSL2 Output:</p> <pre><code>\ud83e\ude7a ENV-DOCTOR DIAGNOSIS\n============================================================\n\n\ud83d\udda5\ufe0f  Environment: WSL2 (GPU forwarding enabled)\n\n\ud83c\udfae GPU Driver\n   \u2705 NVIDIA Driver: 535.146.02 (via WSL forwarding)\n   \u2514\u2500 Max CUDA: 12.2\n\n\u2705 GPU forwarding is working correctly!\n</code></pre>"},{"location":"guides/wsl2/#common-issues","title":"Common Issues","text":""},{"location":"guides/wsl2/#driver-installed-inside-wsl","title":"Driver Installed Inside WSL","text":"<p>Problem:</p> <pre><code>\u274c NVIDIA driver installed inside WSL. This breaks GPU forwarding.\n</code></pre> <p>Cause: You installed <code>nvidia-driver-*</code> packages inside WSL2.</p> <p>Fix:</p> <pre><code>sudo apt remove --purge nvidia-* libnvidia-*\nsudo apt autoremove\n</code></pre> <p>Then restart WSL:</p> <pre><code># In PowerShell (Windows)\nwsl --shutdown\n</code></pre>"},{"location":"guides/wsl2/#missing-cuda-libraries","title":"Missing CUDA Libraries","text":"<p>Problem:</p> <pre><code>\u274c Missing /usr/lib/wsl/lib/libcuda.so\n</code></pre> <p>Cause: Windows NVIDIA driver not properly installed or outdated.</p> <p>Fix:</p> <ol> <li>Update Windows NVIDIA driver to 470.76 or newer</li> <li>Download from nvidia.com/drivers</li> <li>Restart Windows after installation</li> </ol>"},{"location":"guides/wsl2/#nvidia-smi-not-working","title":"nvidia-smi Not Working","text":"<p>Problem:</p> <pre><code>\u274c nvidia-smi command failed\n</code></pre> <p>Possible causes:</p> <ol> <li>Windows driver too old - Update to 470.76+</li> <li>WSL version outdated - Run <code>wsl --update</code></li> <li>Hyper-V not enabled - Required for WSL2 GPU</li> </ol> <p>Fix:</p> <pre><code># Update WSL\nwsl --update\n\n# Verify WSL2 version\nwsl --version\n</code></pre>"},{"location":"guides/wsl2/#running-in-wsl1","title":"Running in WSL1","text":"<p>Problem:</p> <pre><code>\u26a0\ufe0f  WSL1 detected. CUDA is not supported in WSL1.\n</code></pre> <p>Fix: Convert to WSL2:</p> <pre><code># List distributions\nwsl --list --verbose\n\n# Convert to WSL2\nwsl --set-version &lt;distro-name&gt; 2\n</code></pre>"},{"location":"guides/wsl2/#requirements","title":"Requirements","text":""},{"location":"guides/wsl2/#windows-requirements","title":"Windows Requirements","text":"<ul> <li>Windows 10 version 21H2 or newer (or Windows 11)</li> <li>NVIDIA driver 470.76 or newer</li> <li>WSL2 (not WSL1)</li> </ul>"},{"location":"guides/wsl2/#wsl2-requirements","title":"WSL2 Requirements","text":"<ul> <li>Do NOT install NVIDIA drivers</li> <li>CUDA toolkit installation is optional (most libraries bundle it)</li> <li>cuDNN can be installed if needed</li> </ul>"},{"location":"guides/wsl2/#recommended-setup","title":"Recommended Setup","text":""},{"location":"guides/wsl2/#minimal-setup-most-users","title":"Minimal Setup (Most Users)","text":"<ol> <li>Install NVIDIA driver on Windows</li> <li>Install WSL2 with Ubuntu</li> <li>Install Python packages normally</li> </ol> <pre><code># In WSL2\npip install torch  # Works out of the box\n</code></pre>"},{"location":"guides/wsl2/#development-setup-for-compilation","title":"Development Setup (For Compilation)","text":"<p>If you need to compile CUDA extensions:</p> <pre><code># Install CUDA toolkit (not driver!)\nwget https://developer.download.nvidia.com/compute/cuda/repos/wsl-ubuntu/x86_64/cuda-keyring_1.1-1_all.deb\nsudo dpkg -i cuda-keyring_1.1-1_all.deb\nsudo apt update\nsudo apt install cuda-toolkit-12-1\n\n# Set environment\nexport CUDA_HOME=/usr/local/cuda-12.1\nexport PATH=$CUDA_HOME/bin:$PATH\n</code></pre>"},{"location":"guides/wsl2/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/wsl2/#check-windows-driver","title":"Check Windows Driver","text":"<p>In PowerShell:</p> <pre><code>nvidia-smi\n</code></pre> <p>Should show driver version 470.76+.</p>"},{"location":"guides/wsl2/#check-wsl2-gpu-access","title":"Check WSL2 GPU Access","text":"<p>In WSL2:</p> <pre><code>nvidia-smi\n</code></pre> <p>Should show same driver as Windows.</p>"},{"location":"guides/wsl2/#check-library-paths","title":"Check Library Paths","text":"<pre><code>ls -la /usr/lib/wsl/lib/\n</code></pre> <p>Should contain <code>libcuda.so</code>, <code>libnvidia-ml.so</code>, etc.</p>"},{"location":"guides/wsl2/#full-debug-output","title":"Full Debug Output","text":"<pre><code>env-doctor debug\n</code></pre> <p>Shows detailed WSL2 detection metadata.</p>"},{"location":"guides/wsl2/#see-also","title":"See Also","text":"<ul> <li>check Command</li> <li>cuda-info Command</li> </ul>"}]}